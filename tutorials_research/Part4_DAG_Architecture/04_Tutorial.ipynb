{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: DAG Architecture - Complex Decision Trees and Graph-Based Workflows\n",
    "\n",
    "This tutorial explores **Directed Acyclic Graphs (DAGs)** in the context of research workflows and agent systems. We'll understand how complex academic research processes can be modeled as DAGs, and how the PHMGA system uses this architecture for signal processing pipelines.\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will understand:\n",
    "1. **DAG Fundamentals**: Core concepts of directed acyclic graphs and their properties\n",
    "2. **Research Pipeline DAGs**: How systematic literature reviews can be modeled as DAGs\n",
    "3. **PHMGA DAG Architecture**: How signal processing workflows use DAG structures\n",
    "4. **Parallel Execution**: Optimizing workflows through parallel node execution\n",
    "5. **Dynamic DAG Construction**: Building adaptive workflows based on intermediate results\n",
    "\n",
    "## ğŸ“š Academic Research Context\n",
    "\n",
    "**Scenario**: You're conducting a **systematic literature review** for your dissertation chapter. The process involves:\n",
    "- Multi-database searches (ArXiv, PubMed, IEEE, ACM)\n",
    "- Parallel screening (title/abstract review)\n",
    "- Quality assessment gates\n",
    "- Conditional branching based on inclusion criteria\n",
    "- Final synthesis and reporting\n",
    "\n",
    "This complex workflow has **dependencies** (can't analyze papers before finding them), **parallel opportunities** (can search multiple databases simultaneously), and **decision points** (include/exclude based on criteria).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Environment Setup\n",
    "\n",
    "Let's set up our environment with all the DAG components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# Add module paths\n",
    "sys.path.append('modules')\n",
    "\n",
    "# Import DAG components\n",
    "from dag_fundamentals import (\n",
    "    ResearchDAG, DAGNode, NodeType, ExecutionStatus,\n",
    "    create_simple_research_dag, demonstrate_dag_fundamentals\n",
    ")\n",
    "from research_pipeline_dag import (\n",
    "    LiteratureReviewDAG, ResearchCriteria, ResearchPhase,\n",
    "    ResearchPipeline\n",
    ")\n",
    "from phm_dag_structure import (\n",
    "    PHMSignalProcessingDAG, SignalProcessingNode, \n",
    "    OperatorCategory, SignalMetadata\n",
    ")\n",
    "\n",
    "print(\"ğŸ•¸ï¸ DAG Architecture Tutorial Environment Ready!\")\n",
    "print(f\"ğŸ•’ Session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Part 4.1: DAG Fundamentals\n",
    "\n",
    "Let's start by understanding the **core concepts** of Directed Acyclic Graphs:\n",
    "\n",
    "### What Makes a Graph a DAG?\n",
    "\n",
    "1. **Directed**: Each edge has a direction (A â†’ B)\n",
    "2. **Acyclic**: No circular paths (prevents infinite loops)\n",
    "3. **Dependencies**: Nodes must wait for their dependencies\n",
    "4. **Topological Order**: There exists a valid execution sequence\n",
    "\n",
    "### Why DAGs for Research Workflows?\n",
    "\n",
    "- **Complex Dependencies**: Some steps must complete before others can start\n",
    "- **Parallel Opportunities**: Independent steps can run simultaneously\n",
    "- **Quality Gates**: Validation points that may halt or redirect workflow\n",
    "- **Resource Optimization**: Efficient allocation of computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate basic DAG concepts\n",
    "print(\"ğŸ”¬ BASIC DAG CONCEPTS DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a simple research DAG\n",
    "simple_dag = create_simple_research_dag()\n",
    "\n",
    "print(\"\\nğŸ“Š Simple Research Workflow Structure:\")\n",
    "print(simple_dag.visualize_structure())\n",
    "\n",
    "print(\"\\nğŸš€ Executing Simple Research Workflow...\")\n",
    "results = simple_dag.execute()\n",
    "\n",
    "print(\"\\nğŸ“ˆ Execution Results:\")\n",
    "for node_id, result in results.items():\n",
    "    node_name = simple_dag.nodes[node_id].name\n",
    "    print(f\"   â€¢ {node_name}: {str(result)[:60]}...\" if len(str(result)) > 60 else f\"   â€¢ {node_name}: {result}\")\n",
    "\n",
    "# Show execution statistics\n",
    "stats = simple_dag.get_statistics()\n",
    "print(f\"\\nğŸ“Š DAG Statistics:\")\n",
    "print(f\"   â€¢ Total nodes: {stats['total_nodes']}\")\n",
    "print(f\"   â€¢ Success rate: {stats['success_rate']:.1%}\")\n",
    "print(f\"   â€¢ Node types: {stats['nodes_by_type']}\")\n",
    "print(f\"   â€¢ Dependencies: {stats['total_dependencies']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`\n",
    "- **Dependency Management**: The DAG automatically determines execution order using topological sorting, ensuring dependencies are satisfied\n",
    "- **Error Handling**: Failed nodes can be handled gracefully without stopping the entire workflow, depending on criticality\n",
    "- **Execution Tracking**: Each node tracks its own execution time and status, enabling performance analysis and debugging\n",
    "`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Part 4.2: Research Pipeline DAGs\n",
    "\n",
    "Now let's build a **complex systematic literature review** workflow using advanced DAG patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive literature review DAG\n",
    "print(\"ğŸ“š SYSTEMATIC LITERATURE REVIEW DAG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define research criteria\n",
    "research_criteria = ResearchCriteria(\n",
    "    inclusion_keywords=[\"machine learning\", \"fault diagnosis\", \"predictive maintenance\"],\n",
    "    exclusion_keywords=[\"obsolete\", \"deprecated\"],\n",
    "    min_publication_year=2020,\n",
    "    minimum_citation_count=5\n",
    ")\n",
    "\n",
    "print(f\"ğŸ¯ Research Topic: Machine Learning for Fault Diagnosis\")\n",
    "print(f\"ğŸ“‹ Inclusion Keywords: {research_criteria.inclusion_keywords}\")\n",
    "print(f\"âŒ Exclusion Keywords: {research_criteria.exclusion_keywords}\")\n",
    "print(f\"ğŸ“… Publication Year: â‰¥ {research_criteria.min_publication_year}\")\n",
    "\n",
    "# Create the literature review DAG\n",
    "review_dag = LiteratureReviewDAG(\"ML_Fault_Diagnosis\", research_criteria)\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ Created Literature Review DAG:\")\n",
    "print(f\"   â€¢ Total nodes: {len(review_dag.nodes)}\")\n",
    "print(f\"   â€¢ Research phases: {len(set(node.research_phase for node in review_dag.nodes.values()))}\")\n",
    "print(f\"   â€¢ Search databases: {len(review_dag.search_databases)}\")\n",
    "\n",
    "# Show the complex DAG structure\n",
    "print(\"\\nğŸ“Š Literature Review DAG Structure:\")\n",
    "print(review_dag.visualize_structure())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the literature review pipeline\n",
    "print(\"\\nğŸš€ Executing Literature Review Pipeline...\")\n",
    "print(\"This demonstrates how complex research workflows can be automated.\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Execute the complete review workflow\n",
    "    review_results = review_dag.execute()\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    print(f\"\\nâœ… Literature Review completed in {execution_time:.2f} seconds\")\n",
    "    \n",
    "    # Show final report\n",
    "    if \"final_report\" in review_results:\n",
    "        report = review_results[\"final_report\"]\n",
    "        print(f\"\\nğŸ“„ Final Research Report:\")\n",
    "        print(f\"   â€¢ Title: {report.get('title', 'N/A')}\")\n",
    "        print(f\"   â€¢ Research Question: {report.get('research_question', 'N/A')[:60]}...\")\n",
    "        \n",
    "        methodology = report.get('methodology', {})\n",
    "        print(f\"   â€¢ Databases Searched: {methodology.get('databases_searched', 0)}\")\n",
    "        print(f\"   â€¢ Papers Found: {methodology.get('total_papers_found', 0)}\")\n",
    "        print(f\"   â€¢ Papers Included: {methodology.get('papers_included', 0)}\")\n",
    "        \n",
    "        key_findings = report.get('key_findings', [])\n",
    "        print(f\"   â€¢ Key Findings ({len(key_findings)}):\")\n",
    "        for i, finding in enumerate(key_findings[:3], 1):\n",
    "            print(f\"     {i}. {finding}\")\n",
    "        \n",
    "        research_gaps = report.get('research_gaps', [])\n",
    "        print(f\"   â€¢ Research Gaps ({len(research_gaps)}):\")\n",
    "        for i, gap in enumerate(research_gaps[:2], 1):\n",
    "            print(f\"     {i}. {gap}\")\n",
    "        \n",
    "        print(f\"   â€¢ Confidence: {report.get('confidence_assessment', 'N/A')}\")\n",
    "    \n",
    "    # Show execution statistics\n",
    "    review_stats = review_dag.get_statistics()\n",
    "    print(f\"\\nğŸ“ˆ Pipeline Statistics:\")\n",
    "    print(f\"   â€¢ Success Rate: {review_stats['success_rate']:.1%}\")\n",
    "    print(f\"   â€¢ Node Distribution: {review_stats['nodes_by_type']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Pipeline execution encountered issues: {e}\")\n",
    "    print(\"This demonstrates how DAGs handle complex workflow failures gracefully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`\n",
    "- **Multi-Phase Workflows**: The literature review DAG spans 6 distinct research phases, each with specialized operations and quality gates\n",
    "- **Parallel Database Searches**: Multiple databases are searched simultaneously, then results are aggregated, demonstrating effective parallelization\n",
    "- **Quality-Driven Decisions**: Quality gates can halt or redirect the workflow based on intermediate results, ensuring research standards\n",
    "`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Part 4.3: PHMGA Signal Processing DAGs\n",
    "\n",
    "Now let's explore how the **PHMGA system** uses DAGs for complex signal processing workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PHMGA signal processing DAG\n",
    "print(\"âš™ï¸ PHMGA SIGNAL PROCESSING DAG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create bearing fault diagnosis pipeline\n",
    "phm_dag = PHMSignalProcessingDAG(\"bearing_fault_diagnosis\")\n",
    "\n",
    "print(f\"ğŸ”§ Created Bearing Fault Diagnosis DAG:\")\n",
    "print(f\"   â€¢ Total processing nodes: {len(phm_dag.nodes)}\")\n",
    "print(f\"   â€¢ Registered operators: {len(phm_dag.operator_registry)}\")\n",
    "print(f\"   â€¢ Analysis type: {phm_dag.analysis_type}\")\n",
    "\n",
    "# Show operator categories\n",
    "operator_categories = {}\n",
    "for op_name, op_spec in phm_dag.operator_registry.items():\n",
    "    category = op_spec.category.value\n",
    "    if category not in operator_categories:\n",
    "        operator_categories[category] = []\n",
    "    operator_categories[category].append(op_name)\n",
    "\n",
    "print(f\"\\nğŸ“‹ Operator Categories:\")\n",
    "for category, operators in operator_categories.items():\n",
    "    print(f\"   â€¢ {category.replace('_', ' ').title()}: {', '.join(operators)}\")\n",
    "\n",
    "# Show DAG structure\n",
    "print(\"\\nğŸ“Š PHM Processing Pipeline Structure:\")\n",
    "print(phm_dag.visualize_structure())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze execution optimization\n",
    "print(\"\\nğŸš€ EXECUTION OPTIMIZATION ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get execution plan\n",
    "execution_plan = phm_dag.get_execution_plan()\n",
    "print(f\"ğŸ“‹ Execution Plan ({len(execution_plan)} operations):\")\n",
    "\n",
    "for i, item in enumerate(execution_plan[:8], 1):  # Show first 8 operations\n",
    "    print(f\"   {i:2d}. {item['operator']:20} | {item['category']:15} | Group {item['parallel_group']} | Cost: {item['estimated_cost']:.1f}\")\n",
    "\n",
    "if len(execution_plan) > 8:\n",
    "    print(f\"   ... and {len(execution_plan) - 8} more operations\")\n",
    "\n",
    "# Analyze optimization potential\n",
    "optimization = phm_dag.optimize_execution()\n",
    "print(f\"\\nâš¡ Optimization Analysis:\")\n",
    "print(f\"   â€¢ Sequential Cost: {optimization['original_sequential_cost']:.1f} units\")\n",
    "print(f\"   â€¢ Parallel Cost: {optimization['optimized_parallel_cost']:.1f} units\")\n",
    "print(f\"   â€¢ Speedup Factor: {optimization['speedup_factor']:.1f}x\")\n",
    "print(f\"   â€¢ Parallel Groups: {len(optimization['parallel_groups'])}\")\n",
    "\n",
    "# Show parallel grouping\n",
    "print(f\"\\nğŸ”„ Parallel Execution Groups:\")\n",
    "for group_id, group_ops in optimization['parallel_groups'].items():\n",
    "    op_names = [op['operator'] for op in group_ops]\n",
    "    max_cost = max(op['estimated_cost'] for op in group_ops)\n",
    "    print(f\"   Group {group_id}: {len(group_ops)} ops (max cost: {max_cost:.1f}) - {', '.join(op_names[:3])}{'...' if len(op_names) > 3 else ''}\")\n",
    "\n",
    "# Memory analysis\n",
    "memory_peaks = optimization['memory_peaks']\n",
    "max_memory = max(peak['memory_usage'] for peak in memory_peaks)\n",
    "print(f\"\\nğŸ’¾ Memory Analysis:\")\n",
    "print(f\"   â€¢ Peak Memory Usage: {max_memory:.1f} units\")\n",
    "print(f\"   â€¢ Memory-Intensive Groups: {len([p for p in memory_peaks if p['memory_usage'] > 3.0])}\")\n",
    "\n",
    "# Optimization suggestions\n",
    "suggestions = optimization['optimization_suggestions']\n",
    "if suggestions:\n",
    "    print(f\"\\nğŸ’¡ Optimization Suggestions:\")\n",
    "    for suggestion in suggestions:\n",
    "        print(f\"   â€¢ {suggestion}\")\n",
    "else:\n",
    "    print(f\"\\nâœ… No immediate optimization suggestions - pipeline is well-balanced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the PHM signal processing pipeline\n",
    "print(\"\\nğŸ” EXECUTING PHM SIGNAL PROCESSING PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"This demonstrates real-time bearing fault diagnosis using DAG-based signal processing...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Execute the complete PHM pipeline\n",
    "    phm_results = phm_dag.execute()\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    print(f\"\\nâœ… PHM Analysis completed in {execution_time:.2f} seconds\")\n",
    "    \n",
    "    # Show signal input information\n",
    "    if \"signal_input\" in phm_results:\n",
    "        signal_info = phm_results[\"signal_input\"]\n",
    "        print(f\"\\nğŸ“Š Signal Input Analysis:\")\n",
    "        print(f\"   â€¢ Signal Shape: {signal_info.get('signal_shape', 'N/A')}\")\n",
    "        print(f\"   â€¢ Sampling Rate: {signal_info.get('sampling_rate', 'N/A')} Hz\")\n",
    "        print(f\"   â€¢ Duration: {signal_info.get('duration', 'N/A')} seconds\")\n",
    "        \n",
    "        metadata = signal_info.get('metadata')\n",
    "        if metadata:\n",
    "            print(f\"   â€¢ Channels: {metadata.channels}\")\n",
    "            print(f\"   â€¢ Signal Type: {metadata.signal_type}\")\n",
    "    \n",
    "    # Show feature extraction results\n",
    "    if \"feature_aggregation\" in phm_results:\n",
    "        features = phm_results[\"feature_aggregation\"]\n",
    "        if \"aggregated_features\" in features:\n",
    "            feat_shape = features[\"aggregated_features\"].shape\n",
    "            print(f\"\\nğŸ”¢ Feature Extraction Results:\")\n",
    "            print(f\"   â€¢ Feature Matrix Shape: {feat_shape}\")\n",
    "            print(f\"   â€¢ Features per Sample: {feat_shape[1] if len(feat_shape) > 1 else 'N/A'}\")\n",
    "            print(f\"   â€¢ Sample Count: {feat_shape[0] if len(feat_shape) > 0 else 'N/A'}\")\n",
    "    \n",
    "    # Show feature selection results\n",
    "    if \"feature_selection\" in phm_results:\n",
    "        selection = phm_results[\"feature_selection\"]\n",
    "        n_selected = selection.get(\"n_features_selected\", 0)\n",
    "        print(f\"\\nğŸ¯ Feature Selection Results:\")\n",
    "        print(f\"   â€¢ Selected Features: {n_selected}\")\n",
    "        if \"selected_indices\" in selection:\n",
    "            indices = selection[\"selected_indices\"]\n",
    "            print(f\"   â€¢ Feature Indices: {list(indices[:5])}{'...' if len(indices) > 5 else ''}\")\n",
    "    \n",
    "    # Show diagnosis results\n",
    "    if \"diagnosis_output\" in phm_results:\n",
    "        diagnoses = phm_results[\"diagnosis_output\"].get(\"diagnoses\", [])\n",
    "        print(f\"\\nğŸ” Fault Diagnosis Results ({len(diagnoses)} samples):\")\n",
    "        \n",
    "        # Show first few diagnoses\n",
    "        for i, diagnosis in enumerate(diagnoses[:3], 1):\n",
    "            print(f\"   Sample {i}:\")\n",
    "            print(f\"      â€¢ Fault Type: {diagnosis.get('fault_type', 'Unknown')}\")\n",
    "            print(f\"      â€¢ Confidence: {diagnosis.get('confidence', 0):.2f}\")\n",
    "            print(f\"      â€¢ Severity: {diagnosis.get('severity', 'Unknown')}\")\n",
    "            print(f\"      â€¢ Recommendation: {diagnosis.get('recommendation', 'N/A')}\")\n",
    "        \n",
    "        if len(diagnoses) > 3:\n",
    "            print(f\"   ... and {len(diagnoses) - 3} more samples\")\n",
    "        \n",
    "        # Aggregate statistics\n",
    "        fault_types = [d.get('fault_type', 'Unknown') for d in diagnoses]\n",
    "        confidences = [d.get('confidence', 0) for d in diagnoses]\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Aggregate Analysis:\")\n",
    "        print(f\"   â€¢ Average Confidence: {np.mean(confidences):.3f}\")\n",
    "        print(f\"   â€¢ High Confidence (>0.8): {sum(1 for c in confidences if c > 0.8)} samples\")\n",
    "        \n",
    "        # Fault distribution\n",
    "        fault_dist = {}\n",
    "        for fault in fault_types:\n",
    "            fault_dist[fault] = fault_dist.get(fault, 0) + 1\n",
    "        print(f\"   â€¢ Fault Distribution: {fault_dist}\")\n",
    "    \n",
    "    # Show validation results\n",
    "    if \"result_validation\" in phm_results:\n",
    "        validation = phm_results[\"result_validation\"]\n",
    "        print(f\"\\nâœ… Validation Summary:\")\n",
    "        print(f\"   â€¢ Total Samples: {validation.get('total_samples', 0)}\")\n",
    "        print(f\"   â€¢ High Confidence Count: {validation.get('high_confidence_count', 0)}\")\n",
    "        print(f\"   â€¢ Average Confidence: {validation.get('average_confidence', 0):.3f}\")\n",
    "        print(f\"   â€¢ Validation Status: {'âœ… Passed' if validation.get('validation_passed', False) else 'âŒ Failed'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Pipeline execution encountered issues: {e}\")\n",
    "    print(\"This demonstrates DAG resilience - individual node failures don't crash the system.\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`\n",
    "- **Multi-Domain Processing**: The PHMGA DAG seamlessly combines time-domain, frequency-domain, and time-frequency analysis in parallel streams\n",
    "- **Dynamic Resource Optimization**: The system automatically calculates optimal parallel execution groups, achieving significant speedup over sequential processing\n",
    "- **Scalable Architecture**: New signal processing operators can be registered and integrated without modifying the core DAG structure\n",
    "`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Part 4.4: Advanced DAG Patterns\n",
    "\n",
    "Let's explore **advanced DAG patterns** used in real-world research and production systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate advanced DAG patterns\n",
    "print(\"ğŸ”„ ADVANCED DAG PATTERNS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Pattern 1: Conditional Branching\n",
    "print(\"\\nğŸŒ³ Pattern 1: Conditional Branching DAGs\")\n",
    "print(\"   Use case: Different analysis paths based on data quality\")\n",
    "\n",
    "def create_conditional_dag():\n",
    "    \"\"\"Create DAG with conditional execution paths\"\"\"\n",
    "    dag = ResearchDAG(\"conditional_analysis\", \"Conditional analysis workflow\")\n",
    "    \n",
    "    def data_quality_check(inputs):\n",
    "        # Simulate quality assessment\n",
    "        quality_score = np.random.random()\n",
    "        return {\n",
    "            \"quality_score\": quality_score,\n",
    "            \"high_quality\": quality_score > 0.7,\n",
    "            \"analysis_path\": \"detailed\" if quality_score > 0.7 else \"basic\"\n",
    "        }\n",
    "    \n",
    "    def detailed_analysis(inputs):\n",
    "        return {\"analysis_type\": \"detailed\", \"result\": \"comprehensive findings\"}\n",
    "    \n",
    "    def basic_analysis(inputs):\n",
    "        return {\"analysis_type\": \"basic\", \"result\": \"preliminary findings\"}\n",
    "    \n",
    "    # Create nodes\n",
    "    input_node = DAGNode(\"input\", \"Data Input\", NodeType.INPUT)\n",
    "    quality_node = DAGNode(\"quality_check\", \"Quality Assessment\", NodeType.DECISION, data_quality_check)\n",
    "    detailed_node = DAGNode(\"detailed\", \"Detailed Analysis\", NodeType.PROCESSING, detailed_analysis)\n",
    "    basic_node = DAGNode(\"basic\", \"Basic Analysis\", NodeType.PROCESSING, basic_analysis)\n",
    "    \n",
    "    # Add to DAG\n",
    "    for node in [input_node, quality_node, detailed_node, basic_node]:\n",
    "        dag.add_node(node)\n",
    "    \n",
    "    # Create conditional edges (in practice, this would be handled by execution logic)\n",
    "    dag.add_edge(\"input\", \"quality_check\")\n",
    "    dag.add_edge(\"quality_check\", \"detailed\")\n",
    "    dag.add_edge(\"quality_check\", \"basic\")\n",
    "    \n",
    "    return dag\n",
    "\n",
    "conditional_dag = create_conditional_dag()\n",
    "print(f\"   â€¢ Created DAG with {len(conditional_dag.nodes)} nodes\")\n",
    "print(f\"   â€¢ Decision nodes: {sum(1 for n in conditional_dag.nodes.values() if n.node_type == NodeType.DECISION)}\")\n",
    "\n",
    "# Pattern 2: Fan-out/Fan-in\n",
    "print(\"\\nğŸ“Š Pattern 2: Fan-out/Fan-in Processing\")\n",
    "print(\"   Use case: Parallel feature extraction followed by aggregation\")\n",
    "\n",
    "fan_out_nodes = []\n",
    "for feature_type in [\"statistical\", \"spectral\", \"temporal\"]:\n",
    "    fan_out_nodes.append(f\"extract_{feature_type}\")\n",
    "\n",
    "print(f\"   â€¢ Fan-out branches: {len(fan_out_nodes)}\")\n",
    "print(f\"   â€¢ Features: {', '.join([f.replace('extract_', '') for f in fan_out_nodes])}\")\n",
    "print(f\"   â€¢ Fan-in aggregation: All branches â†’ feature_fusion\")\n",
    "\n",
    "# Pattern 3: Pipeline Composition\n",
    "print(\"\\nğŸ”— Pattern 3: Pipeline Composition\")\n",
    "print(\"   Use case: Chaining multiple specialized DAGs\")\n",
    "\n",
    "pipeline_stages = [\n",
    "    \"Data Preprocessing Pipeline\",\n",
    "    \"Feature Engineering Pipeline\", \n",
    "    \"Model Training Pipeline\",\n",
    "    \"Validation Pipeline\",\n",
    "    \"Deployment Pipeline\"\n",
    "]\n",
    "\n",
    "print(f\"   â€¢ Pipeline stages: {len(pipeline_stages)}\")\n",
    "for i, stage in enumerate(pipeline_stages, 1):\n",
    "    print(f\"     {i}. {stage}\")\n",
    "\n",
    "# Pattern 4: Error Recovery\n",
    "print(\"\\nğŸ›¡ï¸ Pattern 4: Error Recovery and Retry Logic\")\n",
    "print(\"   Use case: Robust workflows with fallback strategies\")\n",
    "\n",
    "recovery_strategies = [\n",
    "    \"Retry with exponential backoff\",\n",
    "    \"Fallback to alternative data source\",\n",
    "    \"Skip non-critical operations\",\n",
    "    \"Route to manual intervention queue\",\n",
    "    \"Use cached results from previous run\"\n",
    "]\n",
    "\n",
    "for strategy in recovery_strategies:\n",
    "    print(f\"   â€¢ {strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Part 4.5: Comparing DAG vs Linear Workflows\n",
    "\n",
    "Let's compare the **advantages of DAG-based workflows** versus traditional linear approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š DAG vs LINEAR WORKFLOW COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate workflow execution times\n",
    "def simulate_linear_workflow():\n",
    "    \"\"\"Simulate sequential execution\"\"\"\n",
    "    operations = [\"search_db1\", \"search_db2\", \"search_db3\", \"filter\", \"analyze\", \"report\"]\n",
    "    execution_times = [2.0, 2.5, 1.8, 1.0, 3.0, 1.5]  # Simulated times\n",
    "    \n",
    "    total_time = sum(execution_times)\n",
    "    return {\"operations\": operations, \"times\": execution_times, \"total_time\": total_time}\n",
    "\n",
    "def simulate_dag_workflow():\n",
    "    \"\"\"Simulate parallel DAG execution\"\"\"\n",
    "    # Parallel groups: [db searches], [filter], [analyze], [report]\n",
    "    parallel_groups = [\n",
    "        {\"operations\": [\"search_db1\", \"search_db2\", \"search_db3\"], \"times\": [2.0, 2.5, 1.8]},\n",
    "        {\"operations\": [\"filter\"], \"times\": [1.0]},\n",
    "        {\"operations\": [\"analyze\"], \"times\": [3.0]},\n",
    "        {\"operations\": [\"report\"], \"times\": [1.5]}\n",
    "    ]\n",
    "    \n",
    "    # DAG time is max time per group (parallel execution)\n",
    "    group_times = [max(group[\"times\"]) for group in parallel_groups]\n",
    "    total_time = sum(group_times)\n",
    "    \n",
    "    return {\"groups\": parallel_groups, \"group_times\": group_times, \"total_time\": total_time}\n",
    "\n",
    "# Run simulations\n",
    "linear_result = simulate_linear_workflow()\n",
    "dag_result = simulate_dag_workflow()\n",
    "\n",
    "# Show comparison\n",
    "print(\"â±ï¸ Execution Time Comparison:\")\n",
    "print(f\"   ğŸ“ˆ Linear Workflow: {linear_result['total_time']:.1f} seconds\")\n",
    "print(f\"   ğŸ•¸ï¸ DAG Workflow: {dag_result['total_time']:.1f} seconds\")\n",
    "speedup = linear_result['total_time'] / dag_result['total_time']\n",
    "print(f\"   âš¡ Speedup Factor: {speedup:.1f}x\")\n",
    "print(f\"   ğŸ’° Time Saved: {linear_result['total_time'] - dag_result['total_time']:.1f} seconds ({((speedup-1)*100):.0f}% improvement)\")\n",
    "\n",
    "print(\"\\nğŸ”„ Execution Breakdown:\")\n",
    "print(\"   Linear (Sequential):\")\n",
    "for op, time in zip(linear_result['operations'], linear_result['times']):\n",
    "    print(f\"      {op}: {time:.1f}s\")\n",
    "\n",
    "print(\"\\n   DAG (Parallel Groups):\")\n",
    "for i, (group, time) in enumerate(zip(dag_result['groups'], dag_result['group_times']), 1):\n",
    "    ops = ', '.join(group['operations'])\n",
    "    print(f\"      Group {i} ({time:.1f}s): {ops}\")\n",
    "\n",
    "# Feature comparison\n",
    "print(\"\\nğŸ†š Feature Comparison:\")\n",
    "comparison_table = {\n",
    "    \"Aspect\": [\"Execution Time\", \"Resource Utilization\", \"Error Recovery\", \"Scalability\", \"Debugging\", \"Complexity\"],\n",
    "    \"Linear Workflow\": [\"Sequential (slower)\", \"Single-threaded\", \"All-or-nothing\", \"Poor\", \"Simple trace\", \"Low\"],\n",
    "    \"DAG Workflow\": [\"Parallel (faster)\", \"Multi-threaded\", \"Graceful degradation\", \"Excellent\", \"Node-level tracking\", \"Higher\"]\n",
    "}\n",
    "\n",
    "for aspect, linear, dag in zip(comparison_table[\"Aspect\"], comparison_table[\"Linear Workflow\"], comparison_table[\"DAG Workflow\"]):\n",
    "    print(f\"   â€¢ {aspect:18}: {linear:20} vs {dag}\")\n",
    "\n",
    "# Use case recommendations\n",
    "print(\"\\nğŸ’¡ Use Case Recommendations:\")\n",
    "print(\"   ğŸ“ˆ Linear Workflows Best For:\")\n",
    "linear_use_cases = [\"Simple sequential processes\", \"Tight step dependencies\", \"Resource-constrained environments\", \"Quick prototyping\"]\n",
    "for use_case in linear_use_cases:\n",
    "    print(f\"      â€¢ {use_case}\")\n",
    "\n",
    "print(\"\\n   ğŸ•¸ï¸ DAG Workflows Best For:\")\n",
    "dag_use_cases = [\"Complex research pipelines\", \"Parallel processing opportunities\", \"Production systems\", \"Error-resilient workflows\"]\n",
    "for use_case in dag_use_cases:\n",
    "    print(f\"      â€¢ {use_case}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Part 4.6: Key Takeaways and Integration\n",
    "\n",
    "Let's summarize the key concepts and see how DAGs integrate across the entire tutorial series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“ PART 4 KEY TAKEAWAYS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "key_concepts = {\n",
    "    \"ğŸ§  DAG Fundamentals\": [\n",
    "        \"Directed edges enforce execution dependencies\",\n",
    "        \"Acyclic nature prevents infinite loops\",\n",
    "        \"Topological sorting determines execution order\",\n",
    "        \"Nodes track execution state and performance\"\n",
    "    ],\n",
    "    \"ğŸ“š Research Applications\": [\n",
    "        \"Systematic literature reviews as complex DAGs\",\n",
    "        \"Multi-phase research with quality gates\",\n",
    "        \"Parallel database searches and aggregation\",\n",
    "        \"Conditional branching based on intermediate results\"\n",
    "    ],\n",
    "    \"âš™ï¸ Signal Processing\": [\n",
    "        \"Multi-domain feature extraction in parallel\", \n",
    "        \"Dynamic operator registration and composition\",\n",
    "        \"Resource optimization and execution planning\",\n",
    "        \"Scalable processing pipeline architecture\"\n",
    "    ],\n",
    "    \"ğŸ”„ Advanced Patterns\": [\n",
    "        \"Conditional execution paths for different scenarios\",\n",
    "        \"Fan-out/Fan-in for parallel processing\",\n",
    "        \"Pipeline composition for complex workflows\",\n",
    "        \"Error recovery and robust execution strategies\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for concept_category, details in key_concepts.items():\n",
    "    print(f\"\\n{concept_category}:\")\n",
    "    for detail in details:\n",
    "        print(f\"   â€¢ {detail}\")\n",
    "\n",
    "# Integration across tutorial series\n",
    "print(\"\\nğŸ”— Integration Across Tutorial Series:\")\n",
    "integration_points = {\n",
    "    \"Part 1 (Foundations)\": \"LLM providers integrate as DAG nodes for intelligent processing\",\n",
    "    \"Part 2 (Multi-agent)\": \"Agent router becomes DAG orchestrator for complex decisions\", \n",
    "    \"Part 3 (Reflection)\": \"Research workflows implemented as reflection-based DAGs\",\n",
    "    \"Part 4 (DAGs)\": \"Complete workflow orchestration with parallel execution\",\n",
    "    \"Part 5 (PHM Case)\": \"Production deployment using DAG-based PHMGA architecture\"\n",
    "}\n",
    "\n",
    "for part, integration in integration_points.items():\n",
    "    print(f\"   â€¢ {part}: {integration}\")\n",
    "\n",
    "# Performance insights\n",
    "print(\"\\nğŸ“ˆ Performance Insights from This Tutorial:\")\n",
    "performance_data = {\n",
    "    \"Literature Review DAG\": f\"{len(review_dag.nodes)} nodes, {len(set(node.research_phase for node in review_dag.nodes.values()))} phases\",\n",
    "    \"PHMGA Processing DAG\": f\"{len(phm_dag.nodes)} nodes, {optimization['speedup_factor']:.1f}x speedup potential\",\n",
    "    \"Parallel Efficiency\": f\"Up to {speedup:.1f}x faster than sequential execution\",\n",
    "    \"Resource Optimization\": f\"Memory peaks managed across {len(optimization['parallel_groups'])} execution groups\"\n",
    "}\n",
    "\n",
    "for metric, value in performance_data.items():\n",
    "    print(f\"   â€¢ {metric}: {value}\")\n",
    "\n",
    "print(f\"\\nğŸš€ Ready for Part 5: PHM Case Study!\")\n",
    "print(\"In the final tutorial, we'll apply all concepts to a complete\")\n",
    "print(\"bearing fault diagnosis system using the PHMGA architecture.\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Tutorial completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`\n",
    "- **Workflow Orchestration**: DAGs provide the foundation for orchestrating complex multi-agent systems, allowing intelligent task routing and resource management\n",
    "- **Scalability Through Structure**: The DAG architecture enables both horizontal scaling (more parallel nodes) and vertical scaling (more powerful individual operations)\n",
    "- **Production-Ready Patterns**: Advanced DAG patterns like error recovery, conditional branching, and resource optimization are essential for real-world deployment\n",
    "`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¬ Exercises for Practice\n",
    "\n",
    "Try these exercises to deepen your understanding of DAG architectures:\n",
    "\n",
    "### Exercise 1: Custom Research DAG\n",
    "Create a DAG for your specific research domain:\n",
    "- Define domain-specific operators\n",
    "- Implement custom quality gates\n",
    "- Add domain-specific validation nodes\n",
    "\n",
    "### Exercise 2: Dynamic DAG Construction\n",
    "Extend the DAG system to support:\n",
    "- Runtime node addition based on intermediate results\n",
    "- Adaptive execution paths based on data characteristics\n",
    "- Dynamic resource allocation optimization\n",
    "\n",
    "### Exercise 3: Error Handling Patterns\n",
    "Implement robust error handling:\n",
    "- Node-level retry mechanisms with exponential backoff\n",
    "- Graceful degradation strategies\n",
    "- Alternative execution paths for failed operations\n",
    "\n",
    "### Exercise 4: Performance Optimization\n",
    "Optimize DAG execution:\n",
    "- Implement caching for expensive operations\n",
    "- Add resource monitoring and adaptive scheduling\n",
    "- Create benchmarking and profiling tools\n",
    "\n",
    "### Exercise 5: Integration with External Systems\n",
    "Connect DAGs to external services:\n",
    "- Database connectors for persistent state\n",
    "- Message queue integration for distributed processing\n",
    "- REST API endpoints for external triggering\n",
    "\n",
    "## ğŸ“š Further Reading\n",
    "\n",
    "- [Apache Airflow Documentation](https://airflow.apache.org/) - Production DAG orchestration\n",
    "- [Prefect Documentation](https://docs.prefect.io/) - Modern workflow management\n",
    "- [Graph Theory Foundations](https://mathworld.wolfram.com/DirectedAcyclicGraph.html)\n",
    "- [Parallel Computing Patterns](https://patterns.parallel-computing.org/)\n",
    "- [Workflow Management Systems Survey](https://arxiv.org/)\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: [Part 5: PHM Case Study](../Part5_PHM_Case1/05_Tutorial.ipynb) - Complete Bearing Fault Diagnosis System"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}