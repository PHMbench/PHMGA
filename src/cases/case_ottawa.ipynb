{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20931361",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14a18e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/lq/LQcode/2_project/PHMBench/PHMGA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lq/.conda/envs/PA/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "os.chdir('/home/lq/LQcode/2_project/PHMBench/PHMGA/')\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from src.agents.dataset_preparer_agent import dataset_preparer_agent\n",
    "from src.agents.execute_agent import execute_agent\n",
    "from src.agents.inquirer_agent import inquirer_agent\n",
    "from src.agents.plan_agent import plan_agent\n",
    "from src.agents.reflect_agent import reflect_agent_node\n",
    "from src.agents.report_agent import report_agent_node\n",
    "from src.agents.shallow_ml_agent import shallow_ml_agent\n",
    "from src.states.phm_states import PHMState\n",
    "import yaml\n",
    "import os\n",
    "from src.utils import load_state, save_state\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b068ba",
   "metadata": {},
   "source": [
    "## reload state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7923cee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading configuration from config/case_exp_ottawa.yaml ---\n",
      "\n",
      "--- Found existing state file at save/exp2.5ottawa/exp2.5_built_state_ottawa.pkl. Skipping builder workflow. ---\n",
      "\n",
      "--- Loading state from save/exp2.5ottawa/exp2.5_built_state_ottawa.pkl ---\n",
      "...done.\n",
      "Successfully loaded state with 23 nodes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "config_path = \"config/case_exp_ottawa.yaml\"\n",
    "print(f\"--- Loading configuration from {config_path} ---\")\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "state_save_path = 'save/exp2.5ottawa/exp2.5_built_state_ottawa.pkl'\n",
    "builder_cfg = config.get('builder', {})\n",
    "min_depth = builder_cfg.get('min_depth', 0)\n",
    "max_depth = builder_cfg.get('max_depth', float('inf'))\n",
    "\n",
    "# --- Check for existing state ---\n",
    "if os.path.exists(state_save_path):\n",
    "    print(f\"\\n--- Found existing state file at {state_save_path}. Skipping builder workflow. ---\")\n",
    "    state = load_state(state_save_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea52bd",
   "metadata": {},
   "source": [
    "## 重新运行了所以不存在rerun rerun todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcce4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading configuration from config/case_exp_ottawa.yaml ---\n",
      "Loading data for IDs: [47007, 47008, 47009, 47016, 47017, 47018, 47019, 47020, 47021, 47028, 47029, 47030, 47031, 47032, 47033, 47040, 47041, 47042]\n",
      "Loading data for IDs: [47010, 47011, 47012, 47013, 47014, 47015, 47022, 47023, 47024, 47025, 47026, 47027, 47034, 47035, 47036, 47037, 47038, 47039]\n"
     ]
    }
   ],
   "source": [
    "# from src.utils import initialize_state\n",
    "# # --- Initialize state if not loaded ---\n",
    "# ottawa_state_config = \"config/case_exp_ottawa.yaml\"\n",
    "# print(f\"--- Loading configuration from {ottawa_state_config} ---\")\n",
    "# with open(ottawa_state_config, 'r') as f:\n",
    "#     ottawa_state_config = yaml.safe_load(f)\n",
    "# ottawa_state = initialize_state(\n",
    "#     user_instruction=ottawa_state_config['user_instruction'],\n",
    "#     metadata_path=ottawa_state_config['metadata_path'],\n",
    "#     h5_path=ottawa_state_config['h5_path'],\n",
    "#     ref_ids= ottawa_state_config['ref_ids'],\n",
    "#     test_ids= ottawa_state_config['test_ids'],\n",
    "#     case_name=\"ottawa\",\n",
    "#     use_window= ottawa_state_config.get('use_window', True),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13263032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state.dag_state.nodes['ch1'] = ottawa_state.dag_state.nodes['ch1']\n",
    "# state.dag_state.nodes['ch2'] = ottawa_state.dag_state.nodes['ch2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcee26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ottawa_state.dag_state.nodes = state.dag_state.nodes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ottawa_state.dag_state.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf7ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [03:02,  3.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# from src.utils.rerun_dag import run_dag_on_new_data\n",
    "\n",
    "# run_dag_on_new_data(\n",
    "#     state=ottawa_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483de703",
   "metadata": {},
   "source": [
    "## 相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97dd5efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Preparing - Creating datasets for ML model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2. Dataset Preparer Agent: Prepare datasets for ML\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStep 2: Preparing - Creating datasets for ML model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m preparer_updates = \u001b[43mdataset_preparer_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m state = state.copy(update=preparer_updates)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOutput: Updated state with datasets.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LQcode/2_project/PHMBench/PHMGA/src/agents/dataset_preparer_agent.py:91\u001b[39m, in \u001b[36mdataset_preparer_agent\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m     88\u001b[39m tst_path = saved.get(\u001b[33m\"\u001b[39m\u001b[33mtst_path\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Build training and test sets using the found labels\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m X_train, y_train = \u001b[43m_build_dataset_from_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m X_test, y_test = _build_dataset_from_features(tst_path, labels_map)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X_train.size == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m X_test.size == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LQcode/2_project/PHMBench/PHMGA/src/agents/dataset_preparer_agent.py:51\u001b[39m, in \u001b[36m_build_dataset_from_features\u001b[39m\u001b[34m(feature_path, labels_map)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sample_id \u001b[38;5;129;01min\u001b[39;00m data.files:\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sample_id \u001b[38;5;129;01min\u001b[39;00m labels_map:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         feature = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     52\u001b[39m         features_list.append(feature.reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m))\n\u001b[32m     53\u001b[39m         labels_list.append(labels_map[sample_id])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PA/lib/python3.11/site-packages/numpy/lib/_npyio_impl.py:254\u001b[39m, in \u001b[36mNpzFile.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magic == \u001b[38;5;28mformat\u001b[39m.MAGIC_PREFIX:\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mbytes\u001b[39m = \u001b[38;5;28mself\u001b[39m.zip.open(key)\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.zip.read(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PA/lib/python3.11/site-packages/numpy/lib/format.py:851\u001b[39m, in \u001b[36mread_array\u001b[39m\u001b[34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[39m\n\u001b[32m    849\u001b[39m             read_count = \u001b[38;5;28mmin\u001b[39m(max_read_count, count - i)\n\u001b[32m    850\u001b[39m             read_size = \u001b[38;5;28mint\u001b[39m(read_count * dtype.itemsize)\n\u001b[32m--> \u001b[39m\u001b[32m851\u001b[39m             data = \u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marray data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    852\u001b[39m             array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[32m    853\u001b[39m                                                      count=read_count)\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fortran_order:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PA/lib/python3.11/site-packages/numpy/lib/format.py:986\u001b[39m, in \u001b[36m_read_bytes\u001b[39m\u001b[34m(fp, size, error_template)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    982\u001b[39m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[32m    983\u001b[39m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[32m    984\u001b[39m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[32m    985\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m986\u001b[39m         r = \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m         data += r\n\u001b[32m    988\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) == size:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PA/lib/python3.11/zipfile.py:966\u001b[39m, in \u001b[36mZipExtFile.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    964\u001b[39m \u001b[38;5;28mself\u001b[39m._offset = \u001b[32m0\u001b[39m\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m n > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eof:\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[32m    968\u001b[39m         \u001b[38;5;28mself\u001b[39m._readbuffer = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PA/lib/python3.11/zipfile.py:1036\u001b[39m, in \u001b[36mZipExtFile._read1\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1034\u001b[39m         data += \u001b[38;5;28mself\u001b[39m._read2(n - \u001b[38;5;28mlen\u001b[39m(data))\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compress_type == ZIP_STORED:\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28mself\u001b[39m._eof = \u001b[38;5;28mself\u001b[39m._compress_left <= \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PA/lib/python3.11/zipfile.py:1066\u001b[39m, in \u001b[36mZipExtFile._read2\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1063\u001b[39m n = \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m.MIN_READ_SIZE)\n\u001b[32m   1064\u001b[39m n = \u001b[38;5;28mmin\u001b[39m(n, \u001b[38;5;28mself\u001b[39m._compress_left)\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fileobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[38;5;28mself\u001b[39m._compress_left -= \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[32m   1068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PA/lib/python3.11/zipfile.py:786\u001b[39m, in \u001b[36m_SharedFile.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    782\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt read from the ZIP file while there \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    783\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mis an open writing handle on it. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    784\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mClose the writing handle before trying to read.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    785\u001b[39m \u001b[38;5;28mself\u001b[39m._file.seek(\u001b[38;5;28mself\u001b[39m._pos)\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m data = \u001b[38;5;28mself\u001b[39m._file.read(n)\n\u001b[32m    787\u001b[39m \u001b[38;5;28mself\u001b[39m._pos = \u001b[38;5;28mself\u001b[39m._file.tell()\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 2. Dataset Preparer Agent: Prepare datasets for ML\n",
    "print(\"\\nStep 2: Preparing - Creating datasets for ML model...\")\n",
    "preparer_updates = dataset_preparer_agent(state)\n",
    "state = state.copy(update=preparer_updates)\n",
    "print(f\"Output: Updated state with datasets.\")\n",
    "\n",
    "datasets = preparer_updates['datasets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b8de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 1: Inquiring - Performing similarity analysis...\")\n",
    "inquirer_updates = inquirer_agent(state, metrics=[\"cosine\", \"euclidean\"])\n",
    "state = state.copy(update=inquirer_updates)\n",
    "print(f\"Output: Updated state with similarity_results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec109896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ensemble: Found 3 models with CV accuracy > 90% for ensembling.\n",
      "--- High-quality models: ['wavelet_transform_03_ch1', 'fft_01_hilbert_envelope_01_ch1', 'mean_03_wavelet_transform_03_ch1']\n"
     ]
    }
   ],
   "source": [
    "result = shallow_ml_agent(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f18d147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_644608/3454917170.py:1: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  state = state.copy(update={'ml_results': result['metrics_markdown']})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving final state to save/exp2.5ottawa/exp2.5_built_state_ottawa.pkl ---\n",
      "\n",
      "--- Saving state to save/exp2.5ottawa/exp2.5_built_state_ottawa.pkl ---\n",
      "...done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = state.copy(update={'ml_results': result['metrics_markdown']})\n",
    "\n",
    "# Save the final state\n",
    "print(f\"\\n--- Saving final state to {state_save_path} ---\")\n",
    "save_state(state, state_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3145fdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|                                            |   accuracy |       f1 |   cv_accuracy |      cv_f1 |   cv_accuracy_std |   cv_f1_std |\\n|:-------------------------------------------|-----------:|---------:|--------------:|-----------:|------------------:|------------:|\\n| hilbert_envelope_01_ch1                    |   0.706539 | 0.709033 |      0.758891 |   0.754649 |          0.050594 |    0.055472 |\\n| hilbert_envelope_02_ch2                    |   0.339629 | 0.338302 |      0.320853 |   0.319765 |          0.007129 |    0.006871 |\\n| wavelet_transform_03_ch1                   |   0.991385 | 0.991385 |      0.988072 |   0.988065 |          0.008678 |    0.008691 |\\n| wavelet_transform_04_ch2                   |   0.341507 | 0.338874 |      0.308924 |   0.304770 |          0.007241 |    0.008161 |\\n| kurtosis_05_ch1                            |   0.392092 | 0.391570 |      0.407334 |   0.407307 |          0.007945 |    0.008217 |\\n| rms_06_ch2                                 |   0.339518 | 0.339495 |      0.335984 |   0.335905 |          0.007096 |    0.007089 |\\n| fft_01_hilbert_envelope_01_ch1             |   0.966424 | 0.966527 |      0.978131 |   0.978082 |          0.013140 |    0.013155 |\\n| fft_02_hilbert_envelope_02_ch2             |   0.341286 | 0.339787 |      0.316877 |   0.316231 |          0.005372 |    0.005857 |\\n| mean_03_wavelet_transform_03_ch1           |   0.994036 | 0.994035 |      0.997460 |   0.997459 |          0.002203 |    0.002205 |\\n| std_04_wavelet_transform_04_ch2            |   0.372322 | 0.371580 |      0.371880 |   0.369775 |          0.025249 |    0.024441 |\\n| spectral_kurtosis_05_ch1                   |   0.392092 | 0.391570 |      0.407334 |   0.407307 |          0.007945 |    0.008217 |\\n| spectral_flatness_06_ch2                   |   0.333444 | 0.167892 |      0.330572 |   0.166595 |          0.004159 |    0.001950 |\\n| coherence_07_ch1_ch2                       |   0.360062 | 0.359801 |      0.369781 |   0.369117 |          0.003902 |    0.004314 |\\n| mean_01_fft_01_hilbert_envelope_01_ch1     |   0.654738 | 0.657363 |      0.678706 |   0.678195 |          0.009204 |    0.010631 |\\n| kurtosis_02_fft_01_hilbert_envelope_01_ch1 |   0.586812 | 0.590621 |      0.529379 |   0.535723 |          0.059563 |    0.056071 |\\n| std_03_fft_02_hilbert_envelope_02_ch2      |   0.329689 | 0.329697 |      0.326927 |   0.326968 |          0.011221 |    0.011365 |\\n| skew_04_fft_02_hilbert_envelope_02_ch2     |   0.330683 | 0.330621 |      0.333554 |   0.333495 |          0.004141 |    0.004046 |\\n| mean_05_coherence_07_ch1_ch2               |   0.332339 | 0.332322 |      0.334769 |   0.334596 |          0.003572 |    0.003538 |\\n| max_06_coherence_07_ch1_ch2                |   0.340623 | 0.340616 |      0.333665 |   0.333521 |          0.003788 |    0.003845 |\\n| crest_factor_07_ch1                        |   0.352883 | 0.351947 |      0.336315 |   0.334518 |          0.021045 |    0.021810 |\\n| shape_factor_08_ch2                        |   0.357301 | 0.357108 |      0.375856 |   0.374888 |          0.003961 |    0.004455 |\\n| ensemble                                   |   0.993042 | 0.993037 |    nan        | nan        |        nan        |  nan        |'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_md = result['metrics_markdown']\n",
    "result_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32334db5",
   "metadata": {},
   "source": [
    "## save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "581a3c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load dataset dict\n",
    "import pickle\n",
    "import os\n",
    "save_dir  = '/mnt/crucial/LQ/save/cache'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'datasets.pkl')\n",
    "# with open(save_path, 'wb') as f:\n",
    "#     pickle.dump(datasets, f)\n",
    "with open(save_path, 'rb') as f:\n",
    "    datasets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b37de8a",
   "metadata": {},
   "source": [
    "## repotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "584107ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Reporting - Generating final analysis report...\n",
      "Generating final report with 23 nodes, 14 leaves, 2 similarity stats, issues: 0\n",
      "\n",
      "--- Report Agent LLM Response ---\n",
      "好的，作为一名PHM（Prognostics and Health Management）领域的报告工程师，我将根据您提供的数据和指令生成最终诊断报告。\n",
      "\n",
      "***\n",
      "\n",
      "# 轴承变转速工况故障诊断分析报告\n",
      "\n",
      "本报告旨在分析所提供的轴承振动信号，以识别潜在的健康状况和故障模式。根据用户指令，本次分析的数据集包含变转速工况下的信号，这意味着故障特征会随转速变化而改变。参考集与测试集均包含三种状态：健康（Health）、内圈故障（Inner Race Fault）和外圈故障（Outer Race Fault）。核心目标是通过将测试信号与参考集进行对比，实现对每个测试信号的精确分类。\n",
      "\n",
      "## 1. 流程概览\n",
      "\n",
      "本次诊断任务的处理流程从两个独立的输入通道（`ch1` 和 `ch2`）采集原始振动信号开始。为了从复杂的变转速信号中提取有效的故障特征，我们设计并执行了一个多层次的信号处理与特征提取流程（DAG）：\n",
      "\n",
      "- **信号预处理**:\n",
      "  - 对两个通道的原始信号 (`ch1`, `ch2`) 分别应用了 **希尔伯特包络（Hilbert Envelope）** 和 **连续小波变换（Continuous Wavelet Transform）**。希尔ber特包络主要用于解调幅值调制信息，而小波变换则非常适合分析变转速这类非平稳信号的时频特性。\n",
      "\n",
      "- **特征提取**:\n",
      "  - **时域与频域统计特征**: 直接从原始信号 `ch1` 和 `ch2` 中提取了多种统计指标，例如峰度（Kurtosis）、峭度因子（Crest Factor）、均方根（RMS）、波形因子（Shape Factor）等。\n",
      "  - **包络谱特征**: 对希尔伯特包络信号进行 **快速傅里叶变换（FFT）**，得到包络谱。并从包络谱中提取了均值（Mean）、标准差（Std）、峰度（Kurtosis）和偏度（Skew）等特征。\n",
      "  - **小波特征**: 对小波变换后的时频矩阵，我们计算了其在不同尺度（Scale）上的能量均值（Mean）和标准差（Std）。\n",
      "  - **多通道特征**: 计算了两个通道间的 **相干性（Coherence）**，并提取了其均值和最大值，以评估通道间的线性相关性。\n",
      "\n",
      "- **分类与诊断**:\n",
      "  - 所有提取出的特征被用于训练和评估一个机器学习分类模型。最终，通过一个 **集成模型（Ensemble Model）** 结合所有特征的优势，对测试信号的健康状态进行最终分类。\n",
      "\n",
      "## 2. 特征洞察\n",
      "\n",
      "本次分析未提供特征间的相似度统计数据（`similarity_stats` 为空），因此无法对特征的区分度进行直接的量化比较。然而，我们可以通过各特征在机器学习模型中的独立表现来间接评估其有效性。\n",
      "\n",
      "- **表现最优的特征**:\n",
      "  - `mean_03_wavelet_transform_03_ch1` (F1-score: 0.994): 基于 **`ch1` 小波变换的均值特征** 取得了近乎完美的分类效果。这表明小波变换成功地捕捉到了与转速变化无关的、稳定的故障模式能量分布，是本次诊断任务中最具区分度的特征。\n",
      "  - `wavelet_transform_03_ch1` (F1-score: 0.991): 完整的小波变换时频图本身也包含极高的信息量，分类准确率极高。\n",
      "  - `fft_01_hilbert_envelope_01_ch1` (F1-score: 0.967): 基于 **`ch1` 包络谱的分析** 也表现出色。这是经典的轴承故障诊断方法，通过解调发现故障引起的周期性冲击，在变转速工况下依然有效。\n",
      "\n",
      "- **表现最差的特征**:\n",
      "  - `spectral_flatness_06_ch2` (F1-score: 0.168): **`ch2` 的谱平坦度** 几乎没有提供任何有用的分类信息。\n",
      "  - 绝大多数来自 `ch2` 的特征（如 `rms_06_ch2`, `hilbert_envelope_02_ch2` 等）以及部分单值统计量（如 `crest_factor_07_ch1`）的 F1 分数均在 0.33 左右，这与三分类问题中的随机猜测水平相当。\n",
      "\n",
      "- **原因推测**:\n",
      "  - **通道 `ch1` 是关键信息源**。无论是小波分析还是包络谱分析，`ch1` 提供的信号质量远高于 `ch2`。这可能与传感器的安装位置有关，`ch1` 的位置更接近故障源，能够更清晰地捕捉到故障冲击。\n",
      "  - **`ch2` 可能存在噪声干扰或信息冗余**。`ch2` 信号的特征区分度普遍较低，表明该通道可能受到严重噪声污染，或者其测量位置无法有效反映轴承的健康状态。\n",
      "  - **高级信号处理方法的优越性**。在变转速工况下，小波变换和包络谱分析等能够处理非平稳信号的方法，显著优于传统的时域统计指标（如RMS、Kurtosis），后者因无法解耦转速变化和故障冲击而失效。\n",
      "\n",
      "## 3. 模型评估\n",
      "\n",
      "为了对轴承的健康状态进行准确分类，我们评估了基于不同特征的机器学习模型性能，并构建了一个集成模型以综合所有特征的诊断能力。详细性能指标如下表所示，其中交叉验证（CV）结果反映了模型在不同数据子集上的泛化能力。\n",
      "\n",
      "| 特征/模型 | Accuracy | F1 Score | CV Accuracy | CV F1 Score | CV Accuracy Std | CV F1 Std |\n",
      "|:---|---:|---:|---:|---:|---:|---:|\n",
      "| hilbert_envelope_01_ch1 | 0.706539 | 0.709033 | 0.758891 | 0.754649 | 0.050594 | 0.055472 |\n",
      "| hilbert_envelope_02_ch2 | 0.339629 | 0.338302 | 0.320853 | 0.319765 | 0.007129 | 0.006871 |\n",
      "| wavelet_transform_03_ch1 | 0.991385 | 0.991385 | 0.988072 | 0.988065 | 0.008678 | 0.008691 |\n",
      "| wavelet_transform_04_ch2 | 0.341507 | 0.338874 | 0.308924 | 0.304770 | 0.007241 | 0.008161 |\n",
      "| kurtosis_05_ch1 | 0.392092 | 0.391570 | 0.407334 | 0.407307 | 0.007945 | 0.008217 |\n",
      "| rms_06_ch2 | 0.339518 | 0.339495 | 0.335984 | 0.335905 | 0.007096 | 0.007089 |\n",
      "| fft_01_hilbert_envelope_01_ch1 | 0.966424 | 0.966527 | 0.978131 | 0.978082 | 0.013140 | 0.013155 |\n",
      "| fft_02_hilbert_envelope_02_ch2 | 0.341286 | 0.339787 | 0.316877 | 0.316231 | 0.005372 | 0.005857 |\n",
      "| mean_03_wavelet_transform_03_ch1 | 0.994036 | 0.994035 | 0.997460 | 0.997459 | 0.002203 | 0.002205 |\n",
      "| std_04_wavelet_transform_04_ch2 | 0.372322 | 0.371580 | 0.371880 | 0.369775 | 0.025249 | 0.024441 |\n",
      "| spectral_kurtosis_05_ch1 | 0.392092 | 0.391570 | 0.407334 | 0.407307 | 0.007945 | 0.008217 |\n",
      "| spectral_flatness_06_ch2 | 0.333444 | 0.167892 | 0.330572 | 0.166595 | 0.004159 | 0.001950 |\n",
      "| coherence_07_ch1_ch2 | 0.360062 | 0.359801 | 0.369781 | 0.369117 | 0.003902 | 0.004314 |\n",
      "| mean_01_fft_01_hilbert_envelope_01_ch1 | 0.654738 | 0.657363 | 0.678706 | 0.678195 | 0.009204 | 0.010631 |\n",
      "| kurtosis_02_fft_01_hilbert_envelope_01_ch1 | 0.586812 | 0.590621 | 0.529379 | 0.535723 | 0.059563 | 0.056071 |\n",
      "| std_03_fft_02_hilbert_envelope_02_ch2 | 0.329689 | 0.329697 | 0.326927 | 0.326968 | 0.011221 | 0.011365 |\n",
      "| skew_04_fft_02_hilbert_envelope_02_ch2 | 0.330683 | 0.330621 | 0.333554 | 0.333495 | 0.004141 | 0.004046 |\n",
      "| mean_05_coherence_07_ch1_ch2 | 0.332339 | 0.332322 | 0.334769 | 0.334596 | 0.003572 | 0.003538 |\n",
      "| max_06_coherence_07_ch1_ch2 | 0.340623 | 0.340616 | 0.333665 | 0.333521 | 0.003788 | 0.003845 |\n",
      "| crest_factor_07_ch1 | 0.352883 | 0.351947 | 0.336315 | 0.334518 | 0.021045 | 0.021810 |\n",
      "| shape_factor_08_ch2 | 0.357301 | 0.357108 | 0.375856 | 0.374888 | 0.003961 | 0.004455 |\n",
      "| **ensemble** | **0.993042** | **0.993037** | **nan** | **nan** | **nan** | **nan** |\n",
      "\n",
      "**评估摘要**:\n",
      "- **集成模型表现卓越**：最终的 **集成模型（Ensemble）** 取得了 **99.3%** 的准确率和F1分数。这一结果证明，通过整合多个维度的特征（尤其是 `ch1` 的小波和包络谱特征），模型能够非常可靠地对轴承在变转速工况下的三种状态（健康、内圈故障、外圈故障）进行区分。\n",
      "- **单特征性能差异巨大**：如前所述，不同特征的诊断能力相差悬殊。这突显了在复杂工况下选择正确信号处理方法和特征的重要性。\n",
      "\n",
      "## 4. 结论与建议\n",
      "\n",
      "**诊断结论**:\n",
      "- 本次分析所采用的诊断流程 **非常成功**。集成模型达到了 99.3% 的分类准确率，表明该方法能够有效、可靠地识别变转速工况下的轴承健康状态及内、外圈故障。\n",
      "- 诊断模型的成功主要归功于对 **`ch1` 信号的深度处理**，特别是 **小波变换** 和 **包络谱分析** 两种技术，它们是应对变转速挑战的关键。\n",
      "\n",
      "**维护建议**:\n",
      "1.  **部署诊断模型**：鉴于集成模型的高精度和高可靠性，建议将此分析流程和模型部署到实际的设备健康监测系统中，用于在线、自动化的故障预警和诊断。\n",
      "2.  **审查 `ch2` 传感器**：通道 `ch2` 及其衍生特征在本轮分析中几乎未提供有效信息。建议对 `ch2` 对应的传感器进行检查，包括其 **物理安装位置、连接线路和自身健康状况**，以确认其是否工作正常或安装位置是否合理。若确认无效，可在未来监测中考虑将其移除，以降低数据采集和计算成本。\n",
      "3.  **优化特征集**：为提高计算效率，未来可考虑精简特征集，仅保留 `wavelet_transform_03_ch1`, `fft_01_hilbert_envelope_01_ch1` 等少数几个表现最优的特征进行建模。这可以在基本不损失精度的情况下，大幅减少计算负担。\n",
      "\n",
      "## 5. Code 代码部分\n",
      "\n",
      "在数据处理流程（DAG）中，小波变换（`wavelet_transform`）等计算密集型算子是潜在的性能瓶颈。以下代码片段展示了如何通过并行化处理来优化该算子，以提高大规模数据集分析的效率。\n",
      "\n",
      "这里我们使用 `joblib` 库对多个信号的连续小波变换（CWT）进行并行计算。\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pywt\n",
      "from joblib import Parallel, delayed\n",
      "\n",
      "def compute_cwt(signal, scales, wavelet_name):\n",
      "    \"\"\"\n",
      "    计算单个信号的连续小波变换。\n",
      "\n",
      "    Args:\n",
      "        signal (np.ndarray): 一维输入信号。\n",
      "        scales (np.ndarray): 小波变换的尺度数组。\n",
      "        wavelet_name (str): 小波函数名称, e.g., 'morl'。\n",
      "\n",
      "    Returns:\n",
      "        np.ndarray: CWT系数矩阵。\n",
      "    \"\"\"\n",
      "    # 确保信号是一维的\n",
      "    if signal.ndim > 1:\n",
      "        signal = signal.flatten()\n",
      "        \n",
      "    coefficients, _ = pywt.cwt(signal, scales, wavelet_name)\n",
      "    return coefficients\n",
      "\n",
      "def optimized_wavelet_transform_on_batch(signals_batch, scales, wavelet_name='morl', n_jobs=-1):\n",
      "    \"\"\"\n",
      "    使用并行计算优化对一批信号的小波变换处理。\n",
      "\n",
      "    Args:\n",
      "        signals_batch (list of np.ndarray or np.ndarray): 包含多个信号的列表或二维数组。\n",
      "        scales (np.ndarray): 小波变换的尺度数组。\n",
      "        wavelet_name (str): 小波函数名称。\n",
      "        n_jobs (int): 并行处理使用的CPU核心数。-1表示使用所有可用核心。\n",
      "\n",
      "    Returns:\n",
      "        list of np.ndarray: 每个信号对应的CWT系数矩阵列表。\n",
      "    \"\"\"\n",
      "    # 使用joblib的Parallel和delayed函数实现并行化\n",
      "    # delayed(compute_cwt) 创建一个函数的“延迟”版本\n",
      "    # Parallel(...) 会在多个CPU核心上并行执行这些延迟的函数调用\n",
      "    cwt_results = Parallel(n_jobs=n_jobs)(\n",
      "        delayed(compute_cwt)(signal, scales, wavelet_name) for signal in signals_batch\n",
      "    )\n",
      "    \n",
      "    return cwt_results\n",
      "\n",
      "# --- 示例 ---\n",
      "# 假设我们有一批信号数据 (例如，100个信号，每个信号长度为4096)\n",
      "# signals_to_process = np.random.rand(100, 4096) \n",
      "# scales_to_use = np.arange(1, 129)\n",
      "\n",
      "# # 调用优化后的函数\n",
      "# # cwt_matrices = optimized_wavelet_transform_on_batch(signals_to_process, scales_to_use)\n",
      "\n",
      "# print(\"通过并行化，小波变换算子的计算效率得到显著提升，特别是在处理大量信号时。\")\n",
      "\n",
      "```\n",
      "**优化说明**：上述代码通过 `joblib.Parallel` 将原本需要串行循环处理的多个信号的小波变换任务，分配到多个CPU核心上并行执行。这种方式可以成倍缩短处理时间，尤其适用于包含大量测试样本的诊断场景，是提升整个DAG执行效率的有效手段。\n",
      "--------------------------------\n",
      "\n",
      "Output: Updated state with the final report.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_663415/1608283778.py:4: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  state = state.copy(update=report_updates)\n"
     ]
    }
   ],
   "source": [
    "# 4. Report Agent: Generate the final report\n",
    "print(\"\\nStep 4: Reporting - Generating final analysis report...\")\n",
    "report_updates = report_agent_node(state)\n",
    "state = state.copy(update=report_updates)\n",
    "print(\"Output: Updated state with the final report.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87366921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03a56f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Workflow Finished ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PHMState' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_final_report\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m report = \u001b[43mgenerate_final_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/mnt/crucial/LQ/save/cache/final_report.md\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinal Report:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(report)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LQcode/2_project/PHMBench/PHMGA/src/utils/__init__.py:425\u001b[39m, in \u001b[36mgenerate_final_report\u001b[39m\u001b[34m(final_state, report_path)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[33;03m保存最终的报告。\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Workflow Finished ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfinal_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mfinal_report\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    426\u001b[39m     report = final_state[\u001b[33m\"\u001b[39m\u001b[33mfinal_report\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    427\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinal Report:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PA/lib/python3.11/site-packages/pydantic/main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'PHMState' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "from src.utils import generate_final_report\n",
    "report = generate_final_report(state,'/mnt/crucial/LQ/save/cache/final_report.md')\n",
    "print(\"\\nFinal Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f42b7",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d68fa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating UMAP visualizations for 21 datasets ---\n",
      "Processing: hilbert_envelope_01_ch1\n",
      "  Saved visualization to /home/lq/LQcode/2_project/PHMBench/PHMGA/save/exp2.5ottawa/hilbert_envelope_01_ch1_umap.png\n",
      "Processing: hilbert_envelope_02_ch2\n",
      "  Saved visualization to /home/lq/LQcode/2_project/PHMBench/PHMGA/save/exp2.5ottawa/hilbert_envelope_02_ch2_umap.png\n",
      "Processing: wavelet_transform_03_ch1\n",
      "  Saved visualization to /home/lq/LQcode/2_project/PHMBench/PHMGA/save/exp2.5ottawa/wavelet_transform_03_ch1_umap.png\n",
      "Processing: wavelet_transform_04_ch2\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法执行代码，已释放会话。请尝试重新启动内核。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法执行代码，已释放会话。请尝试重新启动内核。. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "from src.utils.visualization import visualize_datasets_umap\n",
    "save_dir = '/home/lq/LQcode/2_project/PHMBench/PHMGA/save/exp2.5ottawa'\n",
    "visualize_datasets_umap(datasets, save_dir=save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
