{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Building Blocks - æ„å»ºæ¨¡å— ğŸ—ï¸\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "é€šè¿‡æœ¬æ•™ç¨‹ï¼Œæ‚¨å°†æŒæ¡ï¼š\n",
    "- ğŸ“Š **çŠ¶æ€ç®¡ç†** - PHMStateå’ŒDAGStateçš„è®¾è®¡æ¨¡å¼\n",
    "- ğŸ”„ **LangGraphå·¥ä½œæµ** - å¤æ‚å›¾ç»“æ„çš„æ„å»ºå’Œæ‰§è¡Œ\n",
    "- ğŸ”€ **Routeræ¨¡å¼** - æ™ºèƒ½è·¯ç”±å’Œè´Ÿè½½å‡è¡¡\n",
    "- ğŸ­ **Providerå·¥å‚** - é«˜çº§LLMé…ç½®å’Œç®¡ç†\n",
    "\n",
    "## ğŸ“‹ é¢„è®¡æ—¶é•¿ï¼š2-3å°æ—¶\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "# è®¾ç½®é¡¹ç›®è·¯å¾„\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—\n",
    "try:\n",
    "    from src.states.phm_states import PHMState, DAGState, InputData\n",
    "    from src.model import get_llm\n",
    "    PROJECT_AVAILABLE = True\n",
    "    print(\"âœ… ä½¿ç”¨çœŸå®é¡¹ç›®ç»„ä»¶\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ é¡¹ç›®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}\")\n",
    "    PROJECT_AVAILABLE = False\n",
    "    print(\"ğŸ”§ å°†ä½¿ç”¨æ¨¡æ‹Ÿç»„ä»¶\")\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. çŠ¶æ€ç®¡ç† - State Management ğŸ“Š\n",
    "\n",
    "### 1.1 TypedDictçŠ¶æ€åŸºç¡€\n",
    "\n",
    "åœ¨Graph Agentä¸­ï¼ŒçŠ¶æ€æ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒã€‚è®©æˆ‘ä»¬ç†è§£çŠ¶æ€ç®¡ç†çš„åŸºæœ¬æ¦‚å¿µï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¡€çŠ¶æ€å®šä¹‰\n",
    "class BasicState(TypedDict):\n",
    "    \"\"\"åŸºç¡€çŠ¶æ€ç¤ºä¾‹\"\"\"\n",
    "    current_step: str\n",
    "    messages: Annotated[List[str], operator.add]  # è‡ªåŠ¨ç´¯åŠ åˆ—è¡¨\n",
    "    sensor_data: Dict[str, float]\n",
    "    analysis_results: Dict[str, Any]\n",
    "    confidence: float\n",
    "\n",
    "# æ¼”ç¤ºçŠ¶æ€æ›´æ–°\n",
    "def demo_state_management():\n",
    "    print(\"ğŸ”„ çŠ¶æ€ç®¡ç†æ¼”ç¤º\")\n",
    "    \n",
    "    # åˆå§‹çŠ¶æ€\n",
    "    initial_state = {\n",
    "        \"current_step\": \"initialization\",\n",
    "        \"messages\": [\"ç³»ç»Ÿå¯åŠ¨\"],\n",
    "        \"sensor_data\": {\"temperature\": 65.2, \"vibration\": 3.1},\n",
    "        \"analysis_results\": {},\n",
    "        \"confidence\": 0.0\n",
    "    }\n",
    "    \n",
    "    print(f\"åˆå§‹çŠ¶æ€: {initial_state}\")\n",
    "    \n",
    "    # çŠ¶æ€æ›´æ–°1\n",
    "    update1 = {\n",
    "        \"current_step\": \"analysis\",\n",
    "        \"messages\": [\"å¼€å§‹æ•°æ®åˆ†æ\"],  # ä¼šè‡ªåŠ¨ç´¯åŠ åˆ°ç°æœ‰messages\n",
    "        \"analysis_results\": {\"frequency_domain\": \"æ­£å¸¸\"}\n",
    "    }\n",
    "    \n",
    "    # æ¨¡æ‹ŸLangGraphçš„çŠ¶æ€åˆå¹¶\n",
    "    merged_state = initial_state.copy()\n",
    "    for key, value in update1.items():\n",
    "        if key == \"messages\":\n",
    "            merged_state[key].extend(value)  # ç´¯åŠ åˆ—è¡¨\n",
    "        elif key == \"analysis_results\":\n",
    "            merged_state[key].update(value)  # æ›´æ–°å­—å…¸\n",
    "        else:\n",
    "            merged_state[key] = value  # ç›´æ¥æ›¿æ¢\n",
    "    \n",
    "    print(f\"\\næ›´æ–°åçŠ¶æ€:\")\n",
    "    for key, value in merged_state.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "demo_state_management()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 PHMä¸“ç”¨çŠ¶æ€ç»“æ„\n",
    "\n",
    "è®©æˆ‘ä»¬å­¦ä¹ PHMç³»ç»Ÿä¸­ä½¿ç”¨çš„ä¸“é—¨çŠ¶æ€ç»“æ„ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_AVAILABLE:\n",
    "    # ä½¿ç”¨çœŸå®çš„PHMState\n",
    "    print(\"ğŸ­ ä½¿ç”¨çœŸå®PHMStateç»“æ„\")\n",
    "    \n",
    "    # åˆ›å»ºç¤ºä¾‹æ•°æ®\n",
    "    import numpy as np\n",
    "    \n",
    "    sample_signal = np.random.randn(1000).reshape(1, -1, 1)\n",
    "    \n",
    "    # åˆ›å»ºInputDataèŠ‚ç‚¹\n",
    "    input_node = InputData(\n",
    "        node_id=\"sensor_data\",\n",
    "        parents=[],\n",
    "        shape=sample_signal.shape,\n",
    "        results={\"signal\": sample_signal},\n",
    "        meta={\"sensor_type\": \"accelerometer\", \"fs\": 1000}\n",
    "    )\n",
    "    \n",
    "    # åˆ›å»ºDAGState\n",
    "    dag_state = DAGState(\n",
    "        user_instruction=\"åˆ†æè½´æ‰¿æŒ¯åŠ¨ä¿¡å·\",\n",
    "        channels=[\"vibration_x\"],\n",
    "        nodes={\"input\": input_node},\n",
    "        leaves=[\"input\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"DAGèŠ‚ç‚¹: {list(dag_state['nodes'].keys())}\")\n",
    "    print(f\"ç”¨æˆ·æŒ‡ä»¤: {dag_state['user_instruction']}\")\n",
    "    print(f\"ä¿¡å·å½¢çŠ¶: {input_node['shape']}\")\n",
    "    \n",
    "else:\n",
    "    # ä½¿ç”¨æ¨¡æ‹Ÿçš„PHMçŠ¶æ€\n",
    "    print(\"ğŸ”§ ä½¿ç”¨æ¨¡æ‹ŸPHMçŠ¶æ€ç»“æ„\")\n",
    "    \n",
    "    class MockPHMState(TypedDict):\n",
    "        \"\"\"æ¨¡æ‹Ÿçš„PHMçŠ¶æ€ç»“æ„\"\"\"\n",
    "        case_name: str\n",
    "        user_instruction: str\n",
    "        sensor_data: Dict[str, Any]\n",
    "        processing_steps: Annotated[List[str], operator.add]\n",
    "        analysis_results: Dict[str, Any]\n",
    "        confidence_scores: Dict[str, float]\n",
    "    \n",
    "    # åˆ›å»ºç¤ºä¾‹çŠ¶æ€\n",
    "    phm_state = {\n",
    "        \"case_name\": \"bearing_diagnosis\",\n",
    "        \"user_instruction\": \"åˆ†æè½´æ‰¿æŒ¯åŠ¨ä¿¡å·ï¼Œæ£€æµ‹æ½œåœ¨æ•…éšœ\",\n",
    "        \"sensor_data\": {\n",
    "            \"accelerometer\": {\"signal\": [1, 2, 3], \"fs\": 1000}\n",
    "        },\n",
    "        \"processing_steps\": [\"æ•°æ®é¢„å¤„ç†\"],\n",
    "        \"analysis_results\": {\"fft_analysis\": \"å®Œæˆ\"},\n",
    "        \"confidence_scores\": {\"overall\": 0.85}\n",
    "    }\n",
    "    \n",
    "    print(\"æ¨¡æ‹ŸPHMçŠ¶æ€åˆ›å»ºå®Œæˆ:\")\n",
    "    for key, value in phm_state.items():\n",
    "        if isinstance(value, dict) and len(str(value)) > 50:\n",
    "            print(f\"  {key}: {type(value).__name__} (å¤æ‚å¯¹è±¡)\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. LangGraphå·¥ä½œæµ - Workflow Management ğŸ”„\n",
    "\n",
    "### 2.1 åˆ›å»ºå¤šèŠ‚ç‚¹å·¥ä½œæµ\n",
    "\n",
    "LangGraphå…è®¸æˆ‘ä»¬åˆ›å»ºå¤æ‚çš„å¤šèŠ‚ç‚¹å·¥ä½œæµï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰å·¥ä½œæµçŠ¶æ€\n",
    "class WorkflowState(TypedDict):\n",
    "    input_data: Dict[str, Any]\n",
    "    processing_history: Annotated[List[str], operator.add]\n",
    "    current_node: str\n",
    "    results: Dict[str, Any]\n",
    "    should_continue: bool\n",
    "\n",
    "# åˆ›å»ºPHMåˆ†æå·¥ä½œæµ\n",
    "class PHMAnalysisWorkflow:\n",
    "    \"\"\"PHMåˆ†æå·¥ä½œæµ\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.workflow = self._build_workflow()\n",
    "    \n",
    "    def _build_workflow(self) -> StateGraph:\n",
    "        \"\"\"æ„å»ºå·¥ä½œæµå›¾\"\"\"\n",
    "        \n",
    "        def preprocessing_node(state: WorkflowState) -> Dict[str, Any]:\n",
    "            \"\"\"æ•°æ®é¢„å¤„ç†èŠ‚ç‚¹\"\"\"\n",
    "            print(\"ğŸ”„ æ‰§è¡Œæ•°æ®é¢„å¤„ç†...\")\n",
    "            \n",
    "            # æ¨¡æ‹Ÿé¢„å¤„ç†é€»è¾‘\n",
    "            raw_data = state[\"input_data\"].get(\"sensor_data\", {})\n",
    "            processed_data = {k: v * 1.1 for k, v in raw_data.items() if isinstance(v, (int, float))}\n",
    "            \n",
    "            return {\n",
    "                \"processing_history\": [\"æ•°æ®é¢„å¤„ç†å®Œæˆ\"],\n",
    "                \"current_node\": \"preprocessing\",\n",
    "                \"results\": {\"processed_data\": processed_data}\n",
    "            }\n",
    "        \n",
    "        def feature_extraction_node(state: WorkflowState) -> Dict[str, Any]:\n",
    "            \"\"\"ç‰¹å¾æå–èŠ‚ç‚¹\"\"\"\n",
    "            print(\"ğŸ” æ‰§è¡Œç‰¹å¾æå–...\")\n",
    "            \n",
    "            # æ¨¡æ‹Ÿç‰¹å¾æå–\n",
    "            processed_data = state[\"results\"].get(\"processed_data\", {})\n",
    "            features = {\n",
    "                \"mean_value\": sum(processed_data.values()) / len(processed_data) if processed_data else 0,\n",
    "                \"max_value\": max(processed_data.values()) if processed_data else 0,\n",
    "                \"feature_count\": len(processed_data)\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"processing_history\": [\"ç‰¹å¾æå–å®Œæˆ\"],\n",
    "                \"current_node\": \"feature_extraction\",\n",
    "                \"results\": {**state[\"results\"], \"features\": features}\n",
    "            }\n",
    "        \n",
    "        def classification_node(state: WorkflowState) -> Dict[str, Any]:\n",
    "            \"\"\"åˆ†ç±»è¯Šæ–­èŠ‚ç‚¹\"\"\"\n",
    "            print(\"ğŸ¯ æ‰§è¡Œæ•…éšœè¯Šæ–­...\")\n",
    "            \n",
    "            features = state[\"results\"].get(\"features\", {})\n",
    "            mean_val = features.get(\"mean_value\", 0)\n",
    "            \n",
    "            # ç®€å•çš„åˆ†ç±»é€»è¾‘\n",
    "            if mean_val > 80:\n",
    "                diagnosis = \"ä¸¥é‡æ•…éšœ\"\n",
    "                confidence = 0.9\n",
    "            elif mean_val > 60:\n",
    "                diagnosis = \"è½»å¾®å¼‚å¸¸\"\n",
    "                confidence = 0.7\n",
    "            else:\n",
    "                diagnosis = \"æ­£å¸¸çŠ¶æ€\"\n",
    "                confidence = 0.95\n",
    "            \n",
    "            return {\n",
    "                \"processing_history\": [f\"è¯Šæ–­å®Œæˆ: {diagnosis} (ç½®ä¿¡åº¦: {confidence:.2f})\"],\n",
    "                \"current_node\": \"classification\",\n",
    "                \"results\": {**state[\"results\"], \"diagnosis\": diagnosis, \"confidence\": confidence},\n",
    "                \"should_continue\": False\n",
    "            }\n",
    "        \n",
    "        # æ„å»ºå›¾\n",
    "        workflow = StateGraph(WorkflowState)\n",
    "        \n",
    "        # æ·»åŠ èŠ‚ç‚¹\n",
    "        workflow.add_node(\"preprocess\", preprocessing_node)\n",
    "        workflow.add_node(\"extract_features\", feature_extraction_node)\n",
    "        workflow.add_node(\"classify\", classification_node)\n",
    "        \n",
    "        # å®šä¹‰æ‰§è¡Œé¡ºåº\n",
    "        workflow.set_entry_point(\"preprocess\")\n",
    "        workflow.add_edge(\"preprocess\", \"extract_features\")\n",
    "        workflow.add_edge(\"extract_features\", \"classify\")\n",
    "        workflow.add_edge(\"classify\", END)\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    def analyze(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"æ‰§è¡Œåˆ†æ\"\"\"\n",
    "        initial_state = {\n",
    "            \"input_data\": input_data,\n",
    "            \"processing_history\": [\"å·¥ä½œæµå¯åŠ¨\"],\n",
    "            \"current_node\": \"start\",\n",
    "            \"results\": {},\n",
    "            \"should_continue\": True\n",
    "        }\n",
    "        \n",
    "        result = self.workflow.invoke(initial_state)\n",
    "        return result\n",
    "\n",
    "# æµ‹è¯•å·¥ä½œæµ\n",
    "print(\"ğŸ­ åˆ›å»ºPHMåˆ†æå·¥ä½œæµ\")\n",
    "phm_workflow = PHMAnalysisWorkflow()\n",
    "\n",
    "# æµ‹è¯•æ•°æ®\n",
    "test_data = {\n",
    "    \"sensor_data\": {\n",
    "        \"temperature\": 75.5,\n",
    "        \"vibration\": 4.2,\n",
    "        \"pressure\": 2.3\n",
    "    },\n",
    "    \"timestamp\": \"2024-01-15 10:30:00\"\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“Š è¾“å…¥æ•°æ®: {test_data}\")\n",
    "result = phm_workflow.analyze(test_data)\n",
    "\n",
    "print(f\"\\nâœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆ\")\n",
    "print(f\"ğŸ”„ æ‰§è¡Œå†å²: {result['processing_history']}\")\n",
    "print(f\"ğŸ¯ æœ€ç»ˆè¯Šæ–­: {result['results'].get('diagnosis', 'N/A')}\")\n",
    "print(f\"ğŸ“ˆ ç½®ä¿¡åº¦: {result['results'].get('confidence', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 æ¡ä»¶åˆ†æ”¯å’Œå¹¶è¡Œå¤„ç†\n",
    "\n",
    "å¤æ‚çš„å·¥ä½œæµéœ€è¦æ¡ä»¶åˆ†æ”¯å’Œå¹¶è¡Œå¤„ç†èƒ½åŠ›ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é«˜çº§å·¥ä½œæµçŠ¶æ€\n",
    "class AdvancedWorkflowState(TypedDict):\n",
    "    sensor_readings: Dict[str, float]\n",
    "    analysis_path: str\n",
    "    processing_steps: Annotated[List[str], operator.add]\n",
    "    parallel_results: Dict[str, Any]\n",
    "    final_decision: str\n",
    "    confidence_level: float\n",
    "\n",
    "class AdvancedPHMWorkflow:\n",
    "    \"\"\"é«˜çº§PHMå·¥ä½œæµï¼Œæ”¯æŒæ¡ä»¶åˆ†æ”¯å’Œå¹¶è¡Œå¤„ç†\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.workflow = self._build_advanced_workflow()\n",
    "    \n",
    "    def _build_advanced_workflow(self) -> StateGraph:\n",
    "        \"\"\"æ„å»ºé«˜çº§å·¥ä½œæµ\"\"\"\n",
    "        \n",
    "        def data_validation_node(state: AdvancedWorkflowState) -> Dict[str, Any]:\n",
    "            \"\"\"æ•°æ®éªŒè¯èŠ‚ç‚¹\"\"\"\n",
    "            readings = state[\"sensor_readings\"]\n",
    "            \n",
    "            # æ£€æŸ¥æ•°æ®è´¨é‡\n",
    "            missing_sensors = []\n",
    "            required_sensors = [\"temperature\", \"vibration\", \"pressure\"]\n",
    "            \n",
    "            for sensor in required_sensors:\n",
    "                if sensor not in readings or readings[sensor] is None:\n",
    "                    missing_sensors.append(sensor)\n",
    "            \n",
    "            if missing_sensors:\n",
    "                analysis_path = \"simple_analysis\"  # ç¼ºå°‘æ•°æ®ï¼Œä½¿ç”¨ç®€å•åˆ†æ\n",
    "                step_msg = f\"æ•°æ®ä¸å®Œæ•´ï¼ˆç¼ºå°‘: {missing_sensors}ï¼‰ï¼Œä½¿ç”¨ç®€å•åˆ†æ\"\n",
    "            else:\n",
    "                analysis_path = \"full_analysis\"  # æ•°æ®å®Œæ•´ï¼Œä½¿ç”¨å®Œæ•´åˆ†æ\n",
    "                step_msg = \"æ•°æ®å®Œæ•´ï¼Œä½¿ç”¨å®Œæ•´åˆ†ææµç¨‹\"\n",
    "            \n",
    "            return {\n",
    "                \"analysis_path\": analysis_path,\n",
    "                \"processing_steps\": [step_msg]\n",
    "            }\n",
    "        \n",
    "        def simple_analysis_node(state: AdvancedWorkflowState) -> Dict[str, Any]:\n",
    "            \"\"\"ç®€å•åˆ†æèŠ‚ç‚¹\"\"\"\n",
    "            readings = state[\"sensor_readings\"]\n",
    "            \n",
    "            # åŸºç¡€åˆ†æé€»è¾‘\n",
    "            avg_value = sum(v for v in readings.values() if v is not None) / len(readings)\n",
    "            \n",
    "            if avg_value > 70:\n",
    "                decision = \"éœ€è¦æ³¨æ„ï¼šä¼ æ„Ÿå™¨è¯»æ•°åé«˜\"\n",
    "                confidence = 0.6\n",
    "            else:\n",
    "                decision = \"çŠ¶æ€æ­£å¸¸\"\n",
    "                confidence = 0.8\n",
    "            \n",
    "            return {\n",
    "                \"processing_steps\": [\"æ‰§è¡Œç®€å•åˆ†æ\"],\n",
    "                \"final_decision\": decision,\n",
    "                \"confidence_level\": confidence\n",
    "            }\n",
    "        \n",
    "        def parallel_analysis_node(state: AdvancedWorkflowState) -> Dict[str, Any]:\n",
    "            \"\"\"å¹¶è¡Œåˆ†æèŠ‚ç‚¹ï¼ˆæ¨¡æ‹Ÿï¼‰\"\"\"\n",
    "            readings = state[\"sensor_readings\"]\n",
    "            \n",
    "            # æ¨¡æ‹Ÿå¹¶è¡Œå¤„ç†çš„å¤šä¸ªåˆ†æ\n",
    "            temp_analysis = {\n",
    "                \"result\": \"æ­£å¸¸\" if readings.get(\"temperature\", 0) < 80 else \"å¼‚å¸¸\",\n",
    "                \"score\": 0.9 if readings.get(\"temperature\", 0) < 80 else 0.3\n",
    "            }\n",
    "            \n",
    "            vibration_analysis = {\n",
    "                \"result\": \"æ­£å¸¸\" if readings.get(\"vibration\", 0) < 5 else \"å¼‚å¸¸\",\n",
    "                \"score\": 0.85 if readings.get(\"vibration\", 0) < 5 else 0.2\n",
    "            }\n",
    "            \n",
    "            pressure_analysis = {\n",
    "                \"result\": \"æ­£å¸¸\" if readings.get(\"pressure\", 0) > 1.5 else \"å¼‚å¸¸\",\n",
    "                \"score\": 0.9 if readings.get(\"pressure\", 0) > 1.5 else 0.4\n",
    "            }\n",
    "            \n",
    "            parallel_results = {\n",
    "                \"temperature\": temp_analysis,\n",
    "                \"vibration\": vibration_analysis,\n",
    "                \"pressure\": pressure_analysis\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"processing_steps\": [\"æ‰§è¡Œå¹¶è¡Œåˆ†æï¼ˆæ¸©åº¦ã€æŒ¯åŠ¨ã€å‹åŠ›ï¼‰\"],\n",
    "                \"parallel_results\": parallel_results\n",
    "            }\n",
    "        \n",
    "        def decision_fusion_node(state: AdvancedWorkflowState) -> Dict[str, Any]:\n",
    "            \"\"\"å†³ç­–èåˆèŠ‚ç‚¹\"\"\"\n",
    "            parallel_results = state[\"parallel_results\"]\n",
    "            \n",
    "            # èåˆå¤šä¸ªåˆ†æç»“æœ\n",
    "            total_score = 0\n",
    "            abnormal_count = 0\n",
    "            \n",
    "            for sensor, analysis in parallel_results.items():\n",
    "                total_score += analysis[\"score\"]\n",
    "                if analysis[\"result\"] == \"å¼‚å¸¸\":\n",
    "                    abnormal_count += 1\n",
    "            \n",
    "            avg_confidence = total_score / len(parallel_results)\n",
    "            \n",
    "            if abnormal_count >= 2:\n",
    "                decision = \"å¤šä¸ªä¼ æ„Ÿå™¨å¼‚å¸¸ï¼Œå»ºè®®ç«‹å³æ£€ä¿®\"\n",
    "            elif abnormal_count == 1:\n",
    "                decision = \"å•ä¸ªä¼ æ„Ÿå™¨å¼‚å¸¸ï¼Œå»ºè®®å¯†åˆ‡ç›‘æ§\"\n",
    "            else:\n",
    "                decision = \"æ‰€æœ‰ä¼ æ„Ÿå™¨æ­£å¸¸\"\n",
    "            \n",
    "            return {\n",
    "                \"processing_steps\": [f\"å†³ç­–èåˆå®Œæˆï¼Œå¼‚å¸¸ä¼ æ„Ÿå™¨æ•°é‡: {abnormal_count}\"],\n",
    "                \"final_decision\": decision,\n",
    "                \"confidence_level\": avg_confidence\n",
    "            }\n",
    "        \n",
    "        # è·¯ç”±å†³ç­–å‡½æ•°\n",
    "        def route_analysis(state: AdvancedWorkflowState) -> str:\n",
    "            \"\"\"æ ¹æ®æ•°æ®è´¨é‡é€‰æ‹©åˆ†æè·¯å¾„\"\"\"\n",
    "            return state[\"analysis_path\"]\n",
    "        \n",
    "        def should_fuse_results(state: AdvancedWorkflowState) -> str:\n",
    "            \"\"\"åˆ¤æ–­æ˜¯å¦éœ€è¦ç»“æœèåˆ\"\"\"\n",
    "            if state.get(\"parallel_results\"):\n",
    "                return \"fusion\"\n",
    "            else:\n",
    "                return \"end\"\n",
    "        \n",
    "        # æ„å»ºå›¾\n",
    "        workflow = StateGraph(AdvancedWorkflowState)\n",
    "        \n",
    "        # æ·»åŠ èŠ‚ç‚¹\n",
    "        workflow.add_node(\"validate\", data_validation_node)\n",
    "        workflow.add_node(\"simple_analysis\", simple_analysis_node)\n",
    "        workflow.add_node(\"parallel_analysis\", parallel_analysis_node)\n",
    "        workflow.add_node(\"fusion\", decision_fusion_node)\n",
    "        \n",
    "        # è®¾ç½®å…¥å£\n",
    "        workflow.set_entry_point(\"validate\")\n",
    "        \n",
    "        # æ¡ä»¶è·¯ç”±\n",
    "        workflow.add_conditional_edges(\n",
    "            \"validate\",\n",
    "            route_analysis,\n",
    "            {\n",
    "                \"simple_analysis\": \"simple_analysis\",\n",
    "                \"full_analysis\": \"parallel_analysis\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # ç®€å•åˆ†æç›´æ¥ç»“æŸ\n",
    "        workflow.add_edge(\"simple_analysis\", END)\n",
    "        \n",
    "        # å¹¶è¡Œåˆ†æåè¿›è¡Œèåˆ\n",
    "        workflow.add_conditional_edges(\n",
    "            \"parallel_analysis\",\n",
    "            should_fuse_results,\n",
    "            {\n",
    "                \"fusion\": \"fusion\",\n",
    "                \"end\": END\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        workflow.add_edge(\"fusion\", END)\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    def run_analysis(self, sensor_readings: Dict[str, float]) -> Dict[str, Any]:\n",
    "        \"\"\"è¿è¡Œé«˜çº§åˆ†æ\"\"\"\n",
    "        initial_state = {\n",
    "            \"sensor_readings\": sensor_readings,\n",
    "            \"analysis_path\": \"\",\n",
    "            \"processing_steps\": [\"é«˜çº§å·¥ä½œæµå¯åŠ¨\"],\n",
    "            \"parallel_results\": {},\n",
    "            \"final_decision\": \"\",\n",
    "            \"confidence_level\": 0.0\n",
    "        }\n",
    "        \n",
    "        result = self.workflow.invoke(initial_state)\n",
    "        return result\n",
    "\n",
    "# æµ‹è¯•é«˜çº§å·¥ä½œæµ\n",
    "print(\"ğŸš€ åˆ›å»ºé«˜çº§PHMå·¥ä½œæµ\")\n",
    "advanced_workflow = AdvancedPHMWorkflow()\n",
    "\n",
    "# æµ‹è¯•ä¸åŒåœºæ™¯\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"name\": \"å®Œæ•´æ•°æ® - æ­£å¸¸çŠ¶æ€\",\n",
    "        \"data\": {\"temperature\": 70, \"vibration\": 3.0, \"pressure\": 2.2}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"å®Œæ•´æ•°æ® - å¤šé¡¹å¼‚å¸¸\",\n",
    "        \"data\": {\"temperature\": 85, \"vibration\": 6.5, \"pressure\": 1.0}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ä¸å®Œæ•´æ•°æ®\",\n",
    "        \"data\": {\"temperature\": 75, \"pressure\": 2.1}  # ç¼ºå°‘æŒ¯åŠ¨æ•°æ®\n",
    "    }\n",
    "]\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ§ª æµ‹è¯•åœºæ™¯: {scenario['name']}\")\n",
    "    print(f\"ğŸ“Š è¾“å…¥æ•°æ®: {scenario['data']}\")\n",
    "    \n",
    "    result = advanced_workflow.run_analysis(scenario['data'])\n",
    "    \n",
    "    print(f\"ğŸ”„ å¤„ç†è·¯å¾„: {result['analysis_path']}\")\n",
    "    print(f\"ğŸ“ å¤„ç†æ­¥éª¤: {result['processing_steps']}\")\n",
    "    print(f\"ğŸ¯ æœ€ç»ˆå†³ç­–: {result['final_decision']}\")\n",
    "    print(f\"ğŸ“ˆ ç½®ä¿¡åº¦: {result['confidence_level']:.2f}\")\n",
    "    \n",
    "    if result.get('parallel_results'):\n",
    "        print(f\"ğŸ”„ å¹¶è¡Œåˆ†æç»“æœ:\")\n",
    "        for sensor, analysis in result['parallel_results'].items():\n",
    "            print(f\"  {sensor}: {analysis['result']} (å¾—åˆ†: {analysis['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Routeræ¨¡å¼ - æ™ºèƒ½è·¯ç”± ğŸ”€\n",
    "\n",
    "### 3.1 åŸºç¡€Routerå®ç°\n",
    "\n",
    "Routeræ¨¡å¼ç”¨äºæ™ºèƒ½åœ°é€‰æ‹©ä¸åŒçš„å¤„ç†è·¯å¾„ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "\n",
    "class AnalysisType(Enum):\n",
    "    QUICK = \"quick\"\n",
    "    STANDARD = \"standard\"\n",
    "    DETAILED = \"detailed\"\n",
    "    EMERGENCY = \"emergency\"\n",
    "\n",
    "class BaseRouter(ABC):\n",
    "    \"\"\"è·¯ç”±å™¨åŸºç±»\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def route(self, data: Dict[str, Any]) -> AnalysisType:\n",
    "        \"\"\"è·¯ç”±å†³ç­–\"\"\"\n",
    "        pass\n",
    "\n",
    "class RuleBasedRouter(BaseRouter):\n",
    "    \"\"\"åŸºäºè§„åˆ™çš„è·¯ç”±å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # å®šä¹‰é˜ˆå€¼\n",
    "        self.emergency_thresholds = {\n",
    "            \"temperature\": 90,\n",
    "            \"vibration\": 8.0,\n",
    "            \"pressure\": 0.5\n",
    "        }\n",
    "        \n",
    "        self.detailed_thresholds = {\n",
    "            \"temperature\": 80,\n",
    "            \"vibration\": 5.0,\n",
    "            \"pressure\": 1.2\n",
    "        }\n",
    "    \n",
    "    def route(self, data: Dict[str, Any]) -> AnalysisType:\n",
    "        \"\"\"åŸºäºè§„åˆ™çš„è·¯ç”±å†³ç­–\"\"\"\n",
    "        sensor_data = data.get(\"sensor_data\", {})\n",
    "        \n",
    "        # æ£€æŸ¥ç´§æ€¥æƒ…å†µ\n",
    "        for sensor, threshold in self.emergency_thresholds.items():\n",
    "            value = sensor_data.get(sensor, 0)\n",
    "            if (sensor == \"pressure\" and value < threshold) or \\\n",
    "               (sensor != \"pressure\" and value > threshold):\n",
    "                return AnalysisType.EMERGENCY\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¯¦ç»†åˆ†æ\n",
    "        detailed_triggers = 0\n",
    "        for sensor, threshold in self.detailed_thresholds.items():\n",
    "            value = sensor_data.get(sensor, 0)\n",
    "            if (sensor == \"pressure\" and value < threshold) or \\\n",
    "               (sensor != \"pressure\" and value > threshold):\n",
    "                detailed_triggers += 1\n",
    "        \n",
    "        if detailed_triggers >= 2:\n",
    "            return AnalysisType.DETAILED\n",
    "        elif detailed_triggers == 1:\n",
    "            return AnalysisType.STANDARD\n",
    "        else:\n",
    "            return AnalysisType.QUICK\n",
    "\n",
    "class LLMBasedRouter(BaseRouter):\n",
    "    \"\"\"åŸºäºLLMçš„æ™ºèƒ½è·¯ç”±å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_provider: str = \"mock\"):\n",
    "        try:\n",
    "            from modules.llm_providers_unified import create_llm\n",
    "            self.llm = create_llm(llm_provider, temperature=0.1)\n",
    "        except ImportError:\n",
    "            print(\"âš ï¸ æ— æ³•å¯¼å…¥LLMï¼Œä½¿ç”¨è§„åˆ™å›é€€\")\n",
    "            self.llm = None\n",
    "            self.rule_router = RuleBasedRouter()\n",
    "    \n",
    "    def route(self, data: Dict[str, Any]) -> AnalysisType:\n",
    "        \"\"\"åŸºäºLLMçš„æ™ºèƒ½è·¯ç”±\"\"\"\n",
    "        if self.llm is None:\n",
    "            return self.rule_router.route(data)\n",
    "        \n",
    "        sensor_data = data.get(\"sensor_data\", {})\n",
    "        user_priority = data.get(\"priority\", \"normal\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "ä½œä¸ºPHMç³»ç»Ÿçš„è·¯ç”±ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¼ æ„Ÿå™¨æ•°æ®å’Œç”¨æˆ·ä¼˜å…ˆçº§ï¼Œé€‰æ‹©æœ€åˆé€‚çš„åˆ†æç±»å‹ï¼š\n",
    "\n",
    "ä¼ æ„Ÿå™¨æ•°æ®: {sensor_data}\n",
    "ç”¨æˆ·ä¼˜å…ˆçº§: {user_priority}\n",
    "\n",
    "å¯é€‰çš„åˆ†æç±»å‹ï¼š\n",
    "- quick: å¿«é€Ÿæ£€æŸ¥ï¼ˆæ­£å¸¸æƒ…å†µï¼‰\n",
    "- standard: æ ‡å‡†åˆ†æï¼ˆè½»å¾®å¼‚å¸¸ï¼‰\n",
    "- detailed: è¯¦ç»†åˆ†æï¼ˆå¤šé¡¹æŒ‡æ ‡å¼‚å¸¸ï¼‰\n",
    "- emergency: ç´§æ€¥åˆ†æï¼ˆä¸¥é‡å¼‚å¸¸ï¼‰\n",
    "\n",
    "è¯·åªå›ç­”åˆ†æç±»å‹ï¼ˆquick/standard/detailed/emergencyï¼‰ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            result = response.content.strip().lower()\n",
    "            \n",
    "            # éªŒè¯LLMå“åº”\n",
    "            for analysis_type in AnalysisType:\n",
    "                if analysis_type.value in result:\n",
    "                    return analysis_type\n",
    "            \n",
    "            # å¦‚æœLLMå“åº”æ— æ•ˆï¼Œä½¿ç”¨è§„åˆ™å›é€€\n",
    "            print(f\"âš ï¸ LLMå“åº”æ— æ•ˆ: {result}ï¼Œä½¿ç”¨è§„åˆ™å›é€€\")\n",
    "            return RuleBasedRouter().route(data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ LLMè·¯ç”±å¤±è´¥: {e}ï¼Œä½¿ç”¨è§„åˆ™å›é€€\")\n",
    "            return RuleBasedRouter().route(data)\n",
    "\n",
    "# è·¯ç”±ç³»ç»Ÿæ¼”ç¤º\n",
    "class RouterDemo:\n",
    "    \"\"\"è·¯ç”±ç³»ç»Ÿæ¼”ç¤º\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rule_router = RuleBasedRouter()\n",
    "        self.llm_router = LLMBasedRouter()\n",
    "    \n",
    "    def demo_routing(self):\n",
    "        \"\"\"æ¼”ç¤ºä¸åŒè·¯ç”±ç­–ç•¥\"\"\"\n",
    "        test_cases = [\n",
    "            {\n",
    "                \"name\": \"æ­£å¸¸çŠ¶æ€\",\n",
    "                \"data\": {\n",
    "                    \"sensor_data\": {\"temperature\": 65, \"vibration\": 2.5, \"pressure\": 2.0},\n",
    "                    \"priority\": \"normal\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"å•é¡¹å¼‚å¸¸\",\n",
    "                \"data\": {\n",
    "                    \"sensor_data\": {\"temperature\": 82, \"vibration\": 3.0, \"pressure\": 2.2},\n",
    "                    \"priority\": \"normal\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"å¤šé¡¹å¼‚å¸¸\",\n",
    "                \"data\": {\n",
    "                    \"sensor_data\": {\"temperature\": 85, \"vibration\": 6.0, \"pressure\": 1.8},\n",
    "                    \"priority\": \"high\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"ç´§æ€¥æƒ…å†µ\",\n",
    "                \"data\": {\n",
    "                    \"sensor_data\": {\"temperature\": 95, \"vibration\": 9.0, \"pressure\": 0.3},\n",
    "                    \"priority\": \"urgent\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(\"ğŸ”€ è·¯ç”±ç³»ç»Ÿæ¼”ç¤º\")\n",
    "        print(f\"{'åœºæ™¯':<15} {'è§„åˆ™è·¯ç”±':<15} {'LLMè·¯ç”±':<15}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for case in test_cases:\n",
    "            rule_result = self.rule_router.route(case[\"data\"])\n",
    "            llm_result = self.llm_router.route(case[\"data\"])\n",
    "            \n",
    "            print(f\"{case['name']:<15} {rule_result.value:<15} {llm_result.value:<15}\")\n",
    "            \n",
    "            # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯\n",
    "            sensor_data = case[\"data\"][\"sensor_data\"]\n",
    "            print(f\"  ğŸ“Š ä¼ æ„Ÿå™¨: T={sensor_data['temperature']}Â°C, V={sensor_data['vibration']}, P={sensor_data['pressure']}bar\")\n",
    "            print()\n",
    "\n",
    "# è¿è¡Œè·¯ç”±æ¼”ç¤º\n",
    "router_demo = RouterDemo()\n",
    "router_demo.demo_routing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. å®è·µç»ƒä¹  ğŸ“\n",
    "\n",
    "### ç»ƒä¹ ï¼šæ„å»ºæ‚¨çš„å¤šèŠ‚ç‚¹å·¥ä½œæµ\n",
    "\n",
    "ç»“åˆæœ¬ç« å­¦åˆ°çš„æ¦‚å¿µï¼Œåˆ›å»ºä¸€ä¸ªå®Œæ•´çš„PHMåˆ†æç³»ç»Ÿï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ ï¼šå®Œå–„è¿™ä¸ªé›†æˆçš„PHMç³»ç»Ÿ\n",
    "class IntegratedPHMSystem:\n",
    "    \"\"\"é›†æˆçš„PHMç³»ç»Ÿ - è¯·å®Œå–„å®ç°\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_provider=\"mock\"):\n",
    "        self.router = LLMBasedRouter(llm_provider)\n",
    "        # TODO: åˆå§‹åŒ–ä¸åŒç±»å‹çš„åˆ†æå·¥ä½œæµ\n",
    "        self.workflows = {\n",
    "            AnalysisType.QUICK: self._create_quick_workflow(),\n",
    "            AnalysisType.STANDARD: self._create_standard_workflow(),\n",
    "            AnalysisType.DETAILED: self._create_detailed_workflow(),\n",
    "            AnalysisType.EMERGENCY: self._create_emergency_workflow()\n",
    "        }\n",
    "    \n",
    "    def _create_quick_workflow(self):\n",
    "        \"\"\"åˆ›å»ºå¿«é€Ÿåˆ†æå·¥ä½œæµ - è¯·å®ç°\"\"\"\n",
    "        # TODO: å®ç°å¿«é€Ÿæ£€æŸ¥å·¥ä½œæµ\n",
    "        def quick_check(state):\n",
    "            return {\"result\": \"å¿«é€Ÿæ£€æŸ¥å®Œæˆ\", \"confidence\": 0.7}\n",
    "        return quick_check\n",
    "    \n",
    "    def _create_standard_workflow(self):\n",
    "        \"\"\"åˆ›å»ºæ ‡å‡†åˆ†æå·¥ä½œæµ - è¯·å®ç°\"\"\"\n",
    "        # TODO: å®ç°æ ‡å‡†åˆ†æå·¥ä½œæµ\n",
    "        def standard_analysis(state):\n",
    "            return {\"result\": \"æ ‡å‡†åˆ†æå®Œæˆ\", \"confidence\": 0.8}\n",
    "        return standard_analysis\n",
    "    \n",
    "    def _create_detailed_workflow(self):\n",
    "        \"\"\"åˆ›å»ºè¯¦ç»†åˆ†æå·¥ä½œæµ - è¯·å®ç°\"\"\"\n",
    "        # TODO: å®ç°è¯¦ç»†åˆ†æå·¥ä½œæµ\n",
    "        def detailed_analysis(state):\n",
    "            return {\"result\": \"è¯¦ç»†åˆ†æå®Œæˆ\", \"confidence\": 0.9}\n",
    "        return detailed_analysis\n",
    "    \n",
    "    def _create_emergency_workflow(self):\n",
    "        \"\"\"åˆ›å»ºç´§æ€¥åˆ†æå·¥ä½œæµ - è¯·å®ç°\"\"\"\n",
    "        # TODO: å®ç°ç´§æ€¥åˆ†æå·¥ä½œæµ\n",
    "        def emergency_analysis(state):\n",
    "            return {\"result\": \"ç´§æ€¥åˆ†æå®Œæˆï¼Œå»ºè®®ç«‹å³åœæœº\", \"confidence\": 0.95}\n",
    "        return emergency_analysis\n",
    "    \n",
    "    def analyze(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"æ‰§è¡Œå®Œæ•´çš„PHMåˆ†æ\"\"\"\n",
    "        # 1. è·¯ç”±å†³ç­–\n",
    "        analysis_type = self.router.route(data)\n",
    "        \n",
    "        # 2. æ‰§è¡Œç›¸åº”çš„å·¥ä½œæµ\n",
    "        workflow = self.workflows[analysis_type]\n",
    "        result = workflow(data)\n",
    "        \n",
    "        # 3. è¿”å›å®Œæ•´ç»“æœ\n",
    "        return {\n",
    "            \"input_data\": data,\n",
    "            \"analysis_type\": analysis_type.value,\n",
    "            \"result\": result[\"result\"],\n",
    "            \"confidence\": result[\"confidence\"]\n",
    "        }\n",
    "\n",
    "# æµ‹è¯•é›†æˆç³»ç»Ÿ\n",
    "print(\"ğŸ­ åˆ›å»ºé›†æˆPHMç³»ç»Ÿ\")\n",
    "integrated_system = IntegratedPHMSystem()\n",
    "\n",
    "# æµ‹è¯•ä¸åŒè¾“å…¥\n",
    "test_inputs = [\n",
    "    {\n",
    "        \"sensor_data\": {\"temperature\": 70, \"vibration\": 3.0, \"pressure\": 2.0},\n",
    "        \"timestamp\": \"2024-01-15 10:00:00\"\n",
    "    },\n",
    "    {\n",
    "        \"sensor_data\": {\"temperature\": 95, \"vibration\": 8.5, \"pressure\": 0.5},\n",
    "        \"priority\": \"urgent\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test_input in enumerate(test_inputs, 1):\n",
    "    print(f\"\\nğŸ§ª æµ‹è¯• {i}: {test_input}\")\n",
    "    result = integrated_system.analyze(test_input)\n",
    "    print(f\"ğŸ“Š åˆ†æç±»å‹: {result['analysis_type']}\")\n",
    "    print(f\"ğŸ¯ åˆ†æç»“æœ: {result['result']}\")\n",
    "    print(f\"ğŸ“ˆ ç½®ä¿¡åº¦: {result['confidence']:.2f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ æŒ‘æˆ˜: å°è¯•å®ç°æ›´å¤æ‚çš„å·¥ä½œæµå’ŒçŠ¶æ€ç®¡ç†ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. æœ¬ç« æ€»ç»“ ğŸ¯\n",
    "\n",
    "### æ‚¨å·²ç»å­¦ä¼šäº†ï¼š\n",
    "\n",
    "âœ… **çŠ¶æ€ç®¡ç†** - TypedDictå’ŒAnnotatedç±»å‹çš„ä½¿ç”¨  \n",
    "âœ… **LangGraphå·¥ä½œæµ** - å¤šèŠ‚ç‚¹ã€æ¡ä»¶åˆ†æ”¯ã€å¹¶è¡Œå¤„ç†  \n",
    "âœ… **Routeræ¨¡å¼** - è§„åˆ™è·¯ç”±å’ŒLLMæ™ºèƒ½è·¯ç”±  \n",
    "âœ… **ç³»ç»Ÿé›†æˆ** - å°†å¤šä¸ªç»„ä»¶ç»„åˆæˆå®Œæ•´ç³»ç»Ÿ  \n",
    "\n",
    "### å…³é”®æ¦‚å¿µå›é¡¾ï¼š\n",
    "\n",
    "- **Annotated[List, operator.add]** - è‡ªåŠ¨ç´¯åŠ åˆ—è¡¨\n",
    "- **æ¡ä»¶è·¯ç”±** - æ ¹æ®çŠ¶æ€åŠ¨æ€é€‰æ‹©æ‰§è¡Œè·¯å¾„\n",
    "- **å¹¶è¡Œå¤„ç†** - å¤šä¸ªåˆ†æåŒæ—¶è¿›è¡Œ\n",
    "- **Routeræ¨¡å¼** - æ™ºèƒ½é€‰æ‹©å¤„ç†ç­–ç•¥\n",
    "\n",
    "### ä¸‹ä¸€æ­¥ï¼š\n",
    "\n",
    "åœ¨ **Part 3** ä¸­ï¼Œæ‚¨å°†å­¦ä¹ ï¼š\n",
    "- ğŸ¤– **ReActæ¨¡å¼** - æ¨ç†-è¡ŒåŠ¨å¾ªç¯çš„é«˜çº§Agent\n",
    "- ğŸ‘¥ **å¤šAgentåä½œ** - Agentå›¢é˜Ÿçš„è®¾è®¡å’Œç®¡ç†\n",
    "- ğŸ› ï¸ **å·¥å…·é›†æˆ** - Agentå¦‚ä½•ä½¿ç”¨å¤–éƒ¨å·¥å…·\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ å‡†å¤‡ç»§ç»­ï¼Ÿ\n",
    "\n",
    "<div style=\"text-align: center; margin: 20px;\">\n",
    "    <a href=\"../Part3_Agent_Architectures/03_Complete_Tutorial.ipynb\" \n",
    "       style=\"background: #FF9800; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px;\">\n",
    "       ğŸ¤– ç»§ç»­ Part 3: Agent Architectures\n",
    "    </a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}