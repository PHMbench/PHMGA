"""
ç»Ÿä¸€LLM Provideræ¥å£æ¨¡å—

æœ¬æ¨¡å—æä¾›äº†ç»Ÿä¸€çš„LLM Provideræ¥å£ï¼Œæ”¯æŒå¤šç§å¤§è¯­è¨€æ¨¡å‹æä¾›å•†ï¼Œ
åŒ…æ‹¬Google Geminiã€OpenAI GPTã€é€šä¹‰åƒé—®ã€æ™ºè°±GLMç­‰ã€‚

ä½¿ç”¨LangChainçš„BaseChatModelæ¥å£ç¡®ä¿ä¸€è‡´æ€§ã€‚
"""

from typing import Optional, Dict, Any, List
import os
import warnings
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import HumanMessage, AIMessage


class UnifiedLLMProvider:
    """ç»Ÿä¸€çš„LLM Providerç®¡ç†å™¨"""
    
    SUPPORTED_PROVIDERS = {
        "google": {
            "package": "langchain_google_genai",
            "class": "ChatGoogleGenerativeAI",
            "default_model": "gemini-pro",
            "api_key_env": "GEMINI_API_KEY",
            "models": ["gemini-pro", "gemini-2.5-pro", "gemini-2.5-flash", "gemini-1.5-pro", "gemini-1.5-flash"]
        },
        "openai": {
            "package": "langchain_openai", 
            "class": "ChatOpenAI",
            "default_model": "gpt-3.5-turbo",
            "api_key_env": "OPENAI_API_KEY",
            "models": ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo", "gpt-4o", "gpt-4o-mini"]
        },
        "tongyi": {
            "package": "langchain_community.llms",
            "class": "Tongyi", 
            "default_model": "qwen-turbo",
            "api_key_env": "DASHSCOPE_API_KEY",
            "models": ["qwen-turbo", "qwen-plus", "qwen-max", "qwen-max-longcontext"]
        },
        "glm": {
            "package": "langchain_community.chat_models",
            "class": "ChatZhipuAI",
            "default_model": "glm-4",
            "api_key_env": "ZHIPUAI_API_KEY", 
            "models": ["glm-4", "glm-4-air", "glm-4-flash", "glm-3-turbo"]
        },
        "mock": {
            "package": "langchain_community.chat_models",
            "class": "FakeListChatModel",
            "default_model": "mock-model",
            "api_key_env": None,
            "models": ["mock-model", "test-model"]
        }
    }
    
    @classmethod
    def create_llm(
        cls,
        provider: str,
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> BaseChatModel:
        """
        åˆ›å»ºLLMå®ä¾‹
        
        å‚æ•°:
            provider: æä¾›å•†åç§° (google, openai, tongyi, glm, mock)
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼‰
            api_key: APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œä»ç¯å¢ƒå˜é‡è·å–ï¼‰
            temperature: æ¸©åº¦å‚æ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        è¿”å›:
            BaseChatModel: LLMå®ä¾‹
        """
        if provider not in cls.SUPPORTED_PROVIDERS:
            raise ValueError(
                f"ä¸æ”¯æŒçš„Provider: {provider}. "
                f"æ”¯æŒçš„Provider: {list(cls.SUPPORTED_PROVIDERS.keys())}"
            )
        
        config = cls.SUPPORTED_PROVIDERS[provider]
        
        # ç¡®å®šæ¨¡å‹åç§°
        if model is None:
            model = config["default_model"]
        elif model not in config["models"]:
            warnings.warn(f"æ¨¡å‹ {model} å¯èƒ½ä¸è¢« {provider} æ”¯æŒ")
        
        # è·å–APIå¯†é’¥
        if api_key is None and config["api_key_env"]:
            api_key = os.getenv(config["api_key_env"])
            if not api_key:
                raise ValueError(
                    f"éœ€è¦APIå¯†é’¥ã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ {config['api_key_env']} "
                    f"æˆ–ç›´æ¥ä¼ å…¥api_keyå‚æ•°"
                )
        
        # åŠ¨æ€å¯¼å…¥å’Œåˆ›å»ºå®ä¾‹
        try:
            if provider == "google":
                from langchain_google_genai import ChatGoogleGenerativeAI
                return ChatGoogleGenerativeAI(
                    model=model,
                    google_api_key=api_key,
                    temperature=temperature,
                    **kwargs
                )
            
            elif provider == "openai":
                from langchain_openai import ChatOpenAI
                return ChatOpenAI(
                    model=model,
                    openai_api_key=api_key,
                    temperature=temperature,
                    **kwargs
                )
            
            elif provider == "tongyi":
                from langchain_community.llms import Tongyi
                return Tongyi(
                    model=model,
                    dashscope_api_key=api_key,
                    temperature=temperature,
                    **kwargs
                )
            
            elif provider == "glm":
                from langchain_community.chat_models import ChatZhipuAI
                return ChatZhipuAI(
                    model=model,
                    api_key=api_key,
                    temperature=temperature,
                    **kwargs
                )
            
            elif provider == "mock":
                from langchain_community.chat_models import FakeListChatModel
                responses = kwargs.get("responses", [
                    "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå“åº”ï¼Œç”¨äºæ¼”ç¤ºå’Œæµ‹è¯•ã€‚",
                    "PHMï¼ˆPrognostics and Health Managementï¼‰æ˜¯é¢„æµ‹æ€§å¥åº·ç®¡ç†æŠ€æœ¯ã€‚",
                    "Graph Agentä½¿ç”¨å›¾ç»“æ„æ¥ç»„ç»‡å’Œæ‰§è¡Œå¤æ‚çš„å·¥ä½œæµç¨‹ã€‚",
                    "LangChainæä¾›äº†ç»Ÿä¸€çš„LLMæ¥å£ï¼Œæ”¯æŒå¤šç§è¯­è¨€æ¨¡å‹ã€‚"
                ])
                return FakeListChatModel(responses=responses)
                
        except ImportError as e:
            raise ImportError(
                f"æ— æ³•å¯¼å…¥ {provider} æ‰€éœ€çš„åŒ…ã€‚è¯·å®‰è£…ï¼š\n"
                f"pip install {cls._get_install_command(provider)}"
            ) from e
    
    @classmethod
    def _get_install_command(cls, provider: str) -> str:
        """è·å–å®‰è£…å‘½ä»¤"""
        install_commands = {
            "google": "langchain-google-genai",
            "openai": "langchain-openai",
            "tongyi": "langchain-community dashscope", 
            "glm": "langchain-community zhipuai",
            "mock": "langchain-community"
        }
        return install_commands.get(provider, "langchain-community")
    
    @classmethod
    def list_available_providers(cls) -> Dict[str, Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„Provider"""
        available = {}
        
        for provider, config in cls.SUPPORTED_PROVIDERS.items():
            # æ£€æŸ¥APIå¯†é’¥æ˜¯å¦å¯ç”¨
            api_key_available = (
                config["api_key_env"] is None or 
                bool(os.getenv(config["api_key_env"]))
            )
            
            # æ£€æŸ¥åŒ…æ˜¯å¦å·²å®‰è£…
            try:
                if provider == "google":
                    import langchain_google_genai
                elif provider == "openai":
                    import langchain_openai
                elif provider == "tongyi":
                    import dashscope
                elif provider == "glm":
                    import zhipuai
                elif provider == "mock":
                    import langchain_community
                package_available = True
            except ImportError:
                package_available = False
            
            available[provider] = {
                "models": config["models"],
                "default_model": config["default_model"],
                "api_key_required": config["api_key_env"] is not None,
                "api_key_available": api_key_available,
                "package_available": package_available,
                "ready": package_available and (not config["api_key_env"] or api_key_available)
            }
        
        return available
    
    @classmethod
    def get_best_available_provider(cls) -> str:
        """è·å–æœ€ä½³å¯ç”¨çš„Provider"""
        available = cls.list_available_providers()
        
        # ä¼˜å…ˆçº§é¡ºåº
        priority_order = ["google", "openai", "tongyi", "glm", "mock"]
        
        for provider in priority_order:
            if available[provider]["ready"]:
                return provider
        
        # å¦‚æœæ²¡æœ‰å¯ç”¨çš„ï¼Œè¿”å›mock
        return "mock"


def create_llm(
    provider: Optional[str] = None,
    model: Optional[str] = None,
    **kwargs
) -> BaseChatModel:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºLLMå®ä¾‹
    
    å¦‚æœä¸æŒ‡å®šproviderï¼Œå°†è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¯ç”¨çš„providerã€‚
    """
    if provider is None:
        provider = UnifiedLLMProvider.get_best_available_provider()
        print(f"ğŸ¤– è‡ªåŠ¨é€‰æ‹©Provider: {provider}")
    
    return UnifiedLLMProvider.create_llm(
        provider=provider,
        model=model,
        **kwargs
    )


class LLMBenchmark:
    """LLM ProvideråŸºå‡†æµ‹è¯•å·¥å…·"""
    
    def __init__(self, providers: Optional[List[str]] = None):
        self.providers = providers or ["google", "openai", "tongyi", "glm", "mock"]
        self.test_queries = [
            "ä»€ä¹ˆæ˜¯PHMï¼ˆé¢„æµ‹æ€§å¥åº·ç®¡ç†ï¼‰ï¼Ÿ",
            "è§£é‡ŠGraph Agentçš„å·¥ä½œåŸç†",
            "LangChainçš„ä¸»è¦ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
    
    def run_benchmark(self) -> Dict[str, Dict[str, Any]]:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        results = {}
        
        for provider in self.providers:
            try:
                llm = create_llm(provider)
                provider_results = self._test_provider(provider, llm)
                results[provider] = provider_results
            except Exception as e:
                results[provider] = {
                    "status": "error",
                    "error": str(e),
                    "avg_response_time": None,
                    "avg_response_length": 0
                }
        
        return results
    
    def _test_provider(self, provider: str, llm: BaseChatModel) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªProvider"""
        import time
        
        response_times = []
        response_lengths = []
        responses = []
        
        for query in self.test_queries:
            try:
                start_time = time.time()
                response = llm.invoke(query)
                end_time = time.time()
                
                response_time = end_time - start_time
                response_content = response.content if hasattr(response, 'content') else str(response)
                response_length = len(response_content)
                
                response_times.append(response_time)
                response_lengths.append(response_length)
                responses.append(response_content[:100] + "..." if len(response_content) > 100 else response_content)
                
            except Exception as e:
                print(f"âŒ {provider} æŸ¥è¯¢å¤±è´¥: {e}")
                continue
        
        if response_times:
            return {
                "status": "success",
                "avg_response_time": sum(response_times) / len(response_times),
                "avg_response_length": sum(response_lengths) / len(response_lengths),
                "sample_responses": responses,
                "test_count": len(response_times)
            }
        else:
            return {
                "status": "failed",
                "error": "æ‰€æœ‰æµ‹è¯•æŸ¥è¯¢éƒ½å¤±è´¥äº†",
                "avg_response_time": None,
                "avg_response_length": 0
            }


def demo_provider_comparison():
    """æ¼”ç¤ºä¸åŒProviderçš„å¯¹æ¯”"""
    print("ğŸš€ LLM Providerå¯¹æ¯”æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ—å‡ºå¯ç”¨çš„Provider
    available = UnifiedLLMProvider.list_available_providers()
    print("\nğŸ“‹ Providerå¯ç”¨æ€§:")
    for provider, info in available.items():
        status = "âœ… å°±ç»ª" if info["ready"] else "âŒ ä¸å¯ç”¨"
        reason = ""
        if not info["ready"]:
            if not info["package_available"]:
                reason = " (ç¼ºå°‘åŒ…)"
            elif not info["api_key_available"]:
                reason = " (ç¼ºå°‘APIå¯†é’¥)"
        
        print(f"  {provider}: {status}{reason}")
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    print("\nğŸ§ª è¿è¡ŒåŸºå‡†æµ‹è¯•...")
    benchmark = LLMBenchmark()
    results = benchmark.run_benchmark()
    
    print("\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ:")
    for provider, result in results.items():
        if result["status"] == "success":
            print(f"\n  ğŸ¤– {provider}:")
            print(f"    å¹³å‡å“åº”æ—¶é—´: {result['avg_response_time']:.2f}ç§’")
            print(f"    å¹³å‡å“åº”é•¿åº¦: {result['avg_response_length']:.0f}å­—ç¬¦")
            print(f"    æµ‹è¯•æˆåŠŸæ•°: {result['test_count']}")
            if result.get("sample_responses"):
                print(f"    ç¤ºä¾‹å“åº”: {result['sample_responses'][0]}")
        else:
            print(f"\n  âŒ {provider}: {result.get('error', 'æµ‹è¯•å¤±è´¥')}")
    
    return results


def interactive_provider_test():
    """äº¤äº’å¼Provideræµ‹è¯•"""
    print("ğŸ® äº¤äº’å¼Provideræµ‹è¯•")
    print("=" * 30)
    
    available_providers = UnifiedLLMProvider.list_available_providers()
    ready_providers = [p for p, info in available_providers.items() if info["ready"]]
    
    if not ready_providers:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„Providerã€‚è¯·é…ç½®APIå¯†é’¥æˆ–ä½¿ç”¨Mock Providerã€‚")
        return
    
    print(f"å¯ç”¨çš„Provider: {', '.join(ready_providers)}")
    
    while True:
        try:
            provider = input(f"\né€‰æ‹©Provider ({'/'.join(ready_providers)}) æˆ– 'quit' é€€å‡º: ").strip()
            
            if provider.lower() == 'quit':
                break
            
            if provider not in ready_providers:
                print(f"âŒ æ— æ•ˆçš„Providerã€‚è¯·é€‰æ‹©: {', '.join(ready_providers)}")
                continue
            
            # åˆ›å»ºLLMå®ä¾‹
            llm = create_llm(provider)
            print(f"âœ… å·²åˆ›å»º {provider} LLMå®ä¾‹")
            
            # äº¤äº’å¼æŸ¥è¯¢
            while True:
                query = input("\nè¾“å…¥æŸ¥è¯¢å†…å®¹ (æˆ– 'back' è¿”å›Provideré€‰æ‹©): ").strip()
                
                if query.lower() == 'back':
                    break
                
                if not query:
                    continue
                
                try:
                    print(f"ğŸ¤– {provider} æ­£åœ¨æ€è€ƒ...")
                    response = llm.invoke(query)
                    content = response.content if hasattr(response, 'content') else str(response)
                    print(f"\nğŸ’¬ å›ç­”:\n{content}\n")
                except Exception as e:
                    print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
        
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    demo_provider_comparison()
    
    # å¯é€‰ï¼šè¿è¡Œäº¤äº’å¼æµ‹è¯•
    # interactive_provider_test()