"""
Graph Agentå…¥é—¨æ¨¡å—

æœ¬æ¨¡å—ä»‹ç»Graph Agentçš„æ ¸å¿ƒæ¦‚å¿µï¼Œå±•ç¤ºä»ä¼ ç»ŸAgentåˆ°Graph Agentçš„æ¼”è¿›ï¼Œ
å¹¶æä¾›åŸºäºLangGraphçš„å®é™…å®ç°ç¤ºä¾‹ã€‚
"""

from typing import Dict, Any, List, TypedDict, Annotated, Optional
import operator
import time
from langgraph.graph import StateGraph, END
from langchain_core.language_models import BaseChatModel
from .llm_providers_unified import create_llm


class SimpleGraphState(TypedDict):
    """ç®€å•GraphçŠ¶æ€å®šä¹‰"""
    messages: Annotated[List[str], operator.add]
    current_step: str
    input_data: Any
    result: Optional[str]
    metadata: Dict[str, Any]


class DiagnosticGraphState(TypedDict):
    """è¯Šæ–­GraphçŠ¶æ€å®šä¹‰"""
    sensor_data: Dict[str, float]
    analysis_result: Optional[str]
    diagnosis: Optional[str] 
    action_plan: Optional[str]
    confidence_score: float
    steps_completed: Annotated[List[str], operator.add]
    timestamp: str


class SimpleGraphAgent:
    """
    ç®€å•çš„Graph Agentå®ç°
    
    æ¼”ç¤ºGraph Agentçš„åŸºæœ¬æ¦‚å¿µï¼š
    - çŠ¶æ€ç®¡ç†
    - èŠ‚ç‚¹å®šä¹‰ 
    - è¾¹å’Œæµç¨‹æ§åˆ¶
    - æ¡ä»¶è·¯ç”±
    """
    
    def __init__(self):
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """æ„å»ºç®€å•çš„Graphå·¥ä½œæµ"""
        
        # å®šä¹‰èŠ‚ç‚¹å‡½æ•°
        def input_node(state: SimpleGraphState) -> SimpleGraphState:
            """è¾“å…¥å¤„ç†èŠ‚ç‚¹"""
            return {
                "messages": ["ğŸ“¥ æ¥æ”¶è¾“å…¥æ•°æ®"],
                "current_step": "input_received",
                "metadata": {"timestamp": time.time()}
            }
        
        def analyze_node(state: SimpleGraphState) -> SimpleGraphState:
            """åˆ†æèŠ‚ç‚¹"""
            input_data = state.get("input_data", "")
            analysis = f"åˆ†æè¾“å…¥: {input_data}" if input_data else "åˆ†æç©ºè¾“å…¥"
            
            return {
                "messages": [f"ğŸ” {analysis}"],
                "current_step": "analysis_complete",
                "result": f"åˆ†æç»“æœ: {analysis}"
            }
        
        def decide_node(state: SimpleGraphState) -> SimpleGraphState:
            """å†³ç­–èŠ‚ç‚¹"""
            return {
                "messages": ["ğŸ¤” åŸºäºåˆ†æç»“æœè¿›è¡Œå†³ç­–"],
                "current_step": "decision_made"
            }
        
        def action_node(state: SimpleGraphState) -> SimpleGraphState:
            """è¡ŒåŠ¨èŠ‚ç‚¹"""
            return {
                "messages": ["ğŸ¬ æ‰§è¡Œå†³ç­–è¡ŒåŠ¨"],
                "current_step": "action_completed",
                "result": "è¡ŒåŠ¨å·²æ‰§è¡Œ"
            }
        
        # åˆ›å»ºStateGraph
        workflow = StateGraph(SimpleGraphState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("input", input_node)
        workflow.add_node("analyze", analyze_node)
        workflow.add_node("decide", decide_node)
        workflow.add_node("action", action_node)
        
        # å®šä¹‰æµç¨‹ï¼ˆè¾¹ï¼‰
        workflow.set_entry_point("input")
        workflow.add_edge("input", "analyze")
        workflow.add_edge("analyze", "decide")
        workflow.add_edge("decide", "action")
        workflow.add_edge("action", END)
        
        return workflow.compile()
    
    def run(self, input_data: Any) -> SimpleGraphState:
        """è¿è¡ŒGraph Agent"""
        initial_state = {
            "messages": [],
            "current_step": "initialized",
            "input_data": input_data,
            "result": None,
            "metadata": {}
        }
        
        result = self.graph.invoke(initial_state)
        return result


class LLMGraphAgent:
    """
    é›†æˆLLMçš„Graph Agent
    
    å±•ç¤ºå¦‚ä½•åœ¨GraphèŠ‚ç‚¹ä¸­ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å†³ç­–
    """
    
    def __init__(self, llm_provider: str = "mock"):
        self.llm = create_llm(llm_provider, temperature=0.7)
        self.graph = self._build_llm_graph()
    
    def _build_llm_graph(self) -> StateGraph:
        """æ„å»ºé›†æˆLLMçš„Graph"""
        
        def llm_analyze_node(state: SimpleGraphState) -> SimpleGraphState:
            """ä½¿ç”¨LLMè¿›è¡Œåˆ†æçš„èŠ‚ç‚¹"""
            input_data = state.get("input_data", "")
            
            prompt = f"""
            ä½œä¸ºä¸€ä¸ªæ™ºèƒ½åˆ†æåŠ©æ‰‹ï¼Œè¯·åˆ†æä»¥ä¸‹è¾“å…¥å¹¶æä¾›æ´å¯Ÿï¼š
            
            è¾“å…¥: {input_data}
            
            è¯·æä¾›ï¼š
            1. å…³é”®ä¿¡æ¯è¯†åˆ«
            2. å¯èƒ½çš„é—®é¢˜æˆ–æœºä¼š
            3. å»ºè®®çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨
            
            è¯·ç”¨ç®€æ´çš„ä¸­æ–‡å›ç­”ã€‚
            """
            
            try:
                response = self.llm.invoke(prompt)
                analysis = response.content if hasattr(response, 'content') else str(response)
            except Exception as e:
                analysis = f"LLMåˆ†æå¤±è´¥: {e}"
            
            return {
                "messages": [f"ğŸ§  LLMåˆ†æå®Œæˆ"],
                "current_step": "llm_analysis_complete",
                "result": analysis
            }
        
        def llm_decide_node(state: SimpleGraphState) -> SimpleGraphState:
            """ä½¿ç”¨LLMè¿›è¡Œå†³ç­–çš„èŠ‚ç‚¹"""
            analysis_result = state.get("result", "")
            
            prompt = f"""
            åŸºäºä»¥ä¸‹åˆ†æç»“æœï¼Œè¯·åšå‡ºå†³ç­–ï¼š
            
            åˆ†æç»“æœ: {analysis_result}
            
            è¯·é€‰æ‹©æœ€åˆé€‚çš„è¡ŒåŠ¨æ–¹æ¡ˆï¼Œå¹¶ç®€è¦è¯´æ˜ç†ç”±ã€‚
            å¯é€‰è¡ŒåŠ¨: ç»§ç»­ç›‘æ§ã€æ·±å…¥è°ƒæŸ¥ã€ç«‹å³å¹²é¢„ã€è¯·æ±‚æ”¯æ´
            
            è¯·ç”¨ç®€æ´çš„ä¸­æ–‡å›ç­”ã€‚
            """
            
            try:
                response = self.llm.invoke(prompt)
                decision = response.content if hasattr(response, 'content') else str(response)
            except Exception as e:
                decision = f"LLMå†³ç­–å¤±è´¥: {e}"
            
            return {
                "messages": [f"ğŸ¯ LLMå†³ç­–å®Œæˆ"],
                "current_step": "llm_decision_made",
                "result": decision
            }
        
        def execute_node(state: SimpleGraphState) -> SimpleGraphState:
            """æ‰§è¡ŒèŠ‚ç‚¹"""
            decision = state.get("result", "")
            
            return {
                "messages": [f"âœ… æ‰§è¡Œå†³ç­–: {decision[:50]}..."],
                "current_step": "execution_complete"
            }
        
        # æ„å»ºGraph
        workflow = StateGraph(SimpleGraphState)
        
        workflow.add_node("llm_analyze", llm_analyze_node)
        workflow.add_node("llm_decide", llm_decide_node)
        workflow.add_node("execute", execute_node)
        
        workflow.set_entry_point("llm_analyze")
        workflow.add_edge("llm_analyze", "llm_decide")
        workflow.add_edge("llm_decide", "execute")
        workflow.add_edge("execute", END)
        
        return workflow.compile()
    
    def run(self, input_data: Any) -> SimpleGraphState:
        """è¿è¡ŒLLM Graph Agent"""
        initial_state = {
            "messages": [],
            "current_step": "initialized",
            "input_data": input_data,
            "result": None,
            "metadata": {"llm_provider": self.llm.__class__.__name__}
        }
        
        result = self.graph.invoke(initial_state)
        return result


class ConditionalGraphAgent:
    """
    å¸¦æ¡ä»¶è·¯ç”±çš„Graph Agent
    
    å±•ç¤ºGraph Agentå¦‚ä½•æ ¹æ®çŠ¶æ€è¿›è¡Œæ¡ä»¶åˆ†æ”¯å’Œå¾ªç¯
    """
    
    def __init__(self, llm_provider: str = "mock"):
        self.llm = create_llm(llm_provider, temperature=0.3)
        self.graph = self._build_conditional_graph()
    
    def _build_conditional_graph(self) -> StateGraph:
        """æ„å»ºå¸¦æ¡ä»¶è·¯ç”±çš„Graph"""
        
        def assess_node(state: DiagnosticGraphState) -> DiagnosticGraphState:
            """è¯„ä¼°èŠ‚ç‚¹"""
            sensor_data = state["sensor_data"]
            
            # è®¡ç®—å¼‚å¸¸åˆ†æ•°
            anomaly_score = 0
            for key, value in sensor_data.items():
                if value > 0.8 or value < 0.2:
                    anomaly_score += abs(value - 0.5) * 2
            
            confidence = min(1.0, anomaly_score)
            
            assessment = f"ä¼ æ„Ÿå™¨è¯„ä¼°å®Œæˆï¼Œå¼‚å¸¸åˆ†æ•°: {anomaly_score:.2f}"
            
            return {
                "analysis_result": assessment,
                "confidence_score": confidence,
                "steps_completed": ["assessment"],
                "timestamp": str(time.time())
            }
        
        def detailed_analysis_node(state: DiagnosticGraphState) -> DiagnosticGraphState:
            """è¯¦ç»†åˆ†æèŠ‚ç‚¹ï¼ˆæ¡ä»¶è§¦å‘ï¼‰"""
            sensor_data = state["sensor_data"]
            
            prompt = f"""
            ä¼ æ„Ÿå™¨æ•°æ®æ˜¾ç¤ºå¼‚å¸¸ï¼Œè¯·è¿›è¡Œè¯¦ç»†åˆ†æï¼š
            
            æ•°æ®: {sensor_data}
            åˆæ­¥è¯„ä¼°: {state.get('analysis_result', '')}
            
            è¯·è¯†åˆ«ï¼š
            1. ä¸»è¦å¼‚å¸¸ç±»å‹
            2. å¯èƒ½çš„æ ¹æœ¬åŸå› 
            3. é£é™©ç­‰çº§è¯„ä¼°
            """
            
            try:
                response = self.llm.invoke(prompt)
                detailed_analysis = response.content if hasattr(response, 'content') else str(response)
            except Exception as e:
                detailed_analysis = f"è¯¦ç»†åˆ†æå¤±è´¥: {e}"
            
            return {
                "diagnosis": detailed_analysis,
                "steps_completed": ["detailed_analysis"]
            }
        
        def quick_check_node(state: DiagnosticGraphState) -> DiagnosticGraphState:
            """å¿«é€Ÿæ£€æŸ¥èŠ‚ç‚¹ï¼ˆæ­£å¸¸æƒ…å†µï¼‰"""
            return {
                "diagnosis": "ç³»ç»ŸçŠ¶æ€æ­£å¸¸ï¼Œæ— éœ€è¿›ä¸€æ­¥è¡ŒåŠ¨",
                "action_plan": "ç»§ç»­ä¾‹è¡Œç›‘æ§",
                "steps_completed": ["quick_check"]
            }
        
        def action_planning_node(state: DiagnosticGraphState) -> DiagnosticGraphState:
            """è¡ŒåŠ¨è§„åˆ’èŠ‚ç‚¹"""
            diagnosis = state.get("diagnosis", "")
            
            prompt = f"""
            åŸºäºè¯Šæ–­ç»“æœåˆ¶å®šè¡ŒåŠ¨è®¡åˆ’ï¼š
            
            è¯Šæ–­: {diagnosis}
            
            è¯·æä¾›å…·ä½“çš„è¡ŒåŠ¨æ­¥éª¤å’Œæ—¶é—´å®‰æ’ã€‚
            """
            
            try:
                response = self.llm.invoke(prompt)
                action_plan = response.content if hasattr(response, 'content') else str(response)
            except Exception as e:
                action_plan = f"è¡ŒåŠ¨è§„åˆ’å¤±è´¥: {e}"
            
            return {
                "action_plan": action_plan,
                "steps_completed": ["action_planning"]
            }
        
        # æ¡ä»¶è·¯ç”±å‡½æ•°
        def route_after_assessment(state: DiagnosticGraphState) -> str:
            """è¯„ä¼°åçš„è·¯ç”±å†³ç­–"""
            confidence = state.get("confidence_score", 0)
            
            if confidence > 0.6:  # é«˜å¼‚å¸¸åˆ†æ•°éœ€è¦è¯¦ç»†åˆ†æ
                return "detailed_analysis"
            else:  # ä½å¼‚å¸¸åˆ†æ•°è¿›è¡Œå¿«é€Ÿæ£€æŸ¥
                return "quick_check"
        
        def route_after_diagnosis(state: DiagnosticGraphState) -> str:
            """è¯Šæ–­åçš„è·¯ç”±å†³ç­–"""
            diagnosis = state.get("diagnosis", "")
            
            if "æ­£å¸¸" in diagnosis:
                return END  # æ­£å¸¸æƒ…å†µç›´æ¥ç»“æŸ
            else:
                return "action_planning"  # å¼‚å¸¸æƒ…å†µéœ€è¦åˆ¶å®šè¡ŒåŠ¨è®¡åˆ’
        
        # æ„å»ºGraph
        workflow = StateGraph(DiagnosticGraphState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("assess", assess_node)
        workflow.add_node("detailed_analysis", detailed_analysis_node)
        workflow.add_node("quick_check", quick_check_node)
        workflow.add_node("action_planning", action_planning_node)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("assess")
        
        # æ¡ä»¶è·¯ç”±
        workflow.add_conditional_edges(
            "assess",
            route_after_assessment,
            {
                "detailed_analysis": "detailed_analysis",
                "quick_check": "quick_check"
            }
        )
        
        # è¯¦ç»†åˆ†æåæ€»æ˜¯éœ€è¦è¡ŒåŠ¨è§„åˆ’
        workflow.add_edge("detailed_analysis", "action_planning")
        
        # å¿«é€Ÿæ£€æŸ¥çš„æ¡ä»¶è·¯ç”±
        workflow.add_conditional_edges(
            "quick_check",
            route_after_diagnosis,
            {
                "action_planning": "action_planning",
                END: END
            }
        )
        
        # è¡ŒåŠ¨è§„åˆ’åç»“æŸ
        workflow.add_edge("action_planning", END)
        
        return workflow.compile()
    
    def run(self, sensor_data: Dict[str, float]) -> DiagnosticGraphState:
        """è¿è¡Œæ¡ä»¶Graph Agent"""
        initial_state = {
            "sensor_data": sensor_data,
            "analysis_result": None,
            "diagnosis": None,
            "action_plan": None,
            "confidence_score": 0.0,
            "steps_completed": [],
            "timestamp": ""
        }
        
        result = self.graph.invoke(initial_state)
        return result


def demonstrate_graph_agents():
    """æ¼”ç¤ºä¸åŒç±»å‹çš„Graph Agent"""
    print("ğŸ•¸ï¸ Graph Agentæ¼”ç¤º")
    print("=" * 50)
    
    # 1. ç®€å•Graph Agent
    print("\n1ï¸âƒ£ ç®€å•Graph Agent:")
    simple_agent = SimpleGraphAgent()
    result1 = simple_agent.run("ç³»ç»Ÿæ¸©åº¦ç›‘æ§æ•°æ®")
    
    print("æ‰§è¡Œæ­¥éª¤:")
    for i, message in enumerate(result1["messages"], 1):
        print(f"  {i}. {message}")
    print(f"æœ€ç»ˆç»“æœ: {result1.get('result', 'N/A')}")
    
    # 2. LLM Graph Agent
    print("\n2ï¸âƒ£ LLMå¢å¼ºGraph Agent:")
    llm_agent = LLMGraphAgent("mock")
    result2 = llm_agent.run("è®¾å¤‡æŒ¯åŠ¨é¢‘ç‡å¼‚å¸¸ï¼Œéœ€è¦è¯Šæ–­åˆ†æ")
    
    print("æ‰§è¡Œæ­¥éª¤:")
    for i, message in enumerate(result2["messages"], 1):
        print(f"  {i}. {message}")
    print(f"LLMå†³ç­–ç»“æœ: {result2.get('result', 'N/A')[:100]}...")
    
    # 3. æ¡ä»¶Graph Agent
    print("\n3ï¸âƒ£ æ¡ä»¶è·¯ç”±Graph Agent:")
    conditional_agent = ConditionalGraphAgent("mock")
    
    # æµ‹è¯•æ­£å¸¸æƒ…å†µ
    normal_data = {"temperature": 0.5, "pressure": 0.4, "vibration": 0.6}
    result3a = conditional_agent.run(normal_data)
    print(f"æ­£å¸¸æ•°æ® {normal_data}:")
    print(f"  æ‰§è¡Œè·¯å¾„: {' -> '.join(result3a['steps_completed'])}")
    print(f"  è¯Šæ–­: {result3a.get('diagnosis', 'N/A')}")
    
    # æµ‹è¯•å¼‚å¸¸æƒ…å†µ
    abnormal_data = {"temperature": 0.9, "pressure": 0.1, "vibration": 0.95}
    result3b = conditional_agent.run(abnormal_data)
    print(f"\\nå¼‚å¸¸æ•°æ® {abnormal_data}:")
    print(f"  æ‰§è¡Œè·¯å¾„: {' -> '.join(result3b['steps_completed'])}")
    print(f"  ç½®ä¿¡åˆ†æ•°: {result3b.get('confidence_score', 0):.2f}")
    print(f"  è¡ŒåŠ¨è®¡åˆ’: {result3b.get('action_plan', 'N/A')[:100]}...")
    
    return result1, result2, result3a, result3b


def compare_traditional_vs_graph():
    """å¯¹æ¯”ä¼ ç»ŸAgentå’ŒGraph Agent"""
    print("\\nâš–ï¸ ä¼ ç»ŸAgent vs Graph Agentå¯¹æ¯”")
    print("=" * 50)
    
    comparison_table = """
    | ç‰¹æ€§ | ä¼ ç»ŸAgent | Graph Agent |
    |------|-----------|-------------|
    | æ‰§è¡Œæµç¨‹ | çº¿æ€§ï¼ˆæ„ŸçŸ¥â†’æ€è€ƒâ†’è¡ŒåŠ¨ï¼‰ | å›¾ç»“æ„ï¼ˆçµæ´»è·¯ç”±ï¼‰ |
    | çŠ¶æ€ç®¡ç† | ç®€å•å˜é‡ | ç»“æ„åŒ–çŠ¶æ€ |
    | å†³ç­–å¤æ‚åº¦ | å•æ­¥å†³ç­– | å¤šæ­¥æ¡ä»¶å†³ç­– |
    | å¹¶è¡Œå¤„ç† | å›°éš¾ | åŸç”Ÿæ”¯æŒ |
    | æµç¨‹å¯è§†åŒ– | éšå« | æ˜ç¡®çš„å›¾ç»“æ„ |
    | æ‰©å±•æ€§ | æœ‰é™ | é«˜åº¦æ¨¡å—åŒ– |
    | è°ƒè¯•èƒ½åŠ› | å›°éš¾ | çŠ¶æ€å¯è¿½è¸ª |
    | é”™è¯¯æ¢å¤ | é‡æ–°å¼€å§‹ | èŠ‚ç‚¹çº§é‡è¯• |
    """
    
    print(comparison_table)
    
    print("\\nğŸ¯ Graph Agentçš„ä¼˜åŠ¿:")
    print("âœ… æ˜ç¡®çš„çŠ¶æ€ç®¡ç†")
    print("âœ… çµæ´»çš„æ¡ä»¶è·¯ç”±") 
    print("âœ… å¯è§†åŒ–çš„å·¥ä½œæµ")
    print("âœ… æ¨¡å—åŒ–çš„èŠ‚ç‚¹è®¾è®¡")
    print("âœ… æ›´å¥½çš„é”™è¯¯å¤„ç†")
    print("âœ… æ”¯æŒå¤æ‚çš„ä¸šåŠ¡é€»è¾‘")
    
    print("\\nâš ï¸ ä½¿ç”¨å»ºè®®:")
    print("â€¢ ç®€å•ä»»åŠ¡ä½¿ç”¨ä¼ ç»ŸAgent")
    print("â€¢ å¤æ‚å·¥ä½œæµä½¿ç”¨Graph Agent")  
    print("â€¢ éœ€è¦æ¡ä»¶åˆ†æ”¯æ—¶é€‰æ‹©Graph Agent")
    print("â€¢ å¤šæ­¥éª¤åä½œåœºæ™¯ä½¿ç”¨Graph Agent")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    demonstrate_graph_agents()
    compare_traditional_vs_graph()