{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 1: Foundations - Graph Agent åŸºç¡€å…¥é—¨\n\n## ğŸ¯ å­¦ä¹ ç›®æ ‡\n\né€šè¿‡æœ¬æ•™ç¨‹ï¼Œæ‚¨å°†æŒæ¡ï¼š\n- ğŸ¤– **ç†è§£AgentåŸºæœ¬æ¦‚å¿µ** - ä»€ä¹ˆæ˜¯æ™ºèƒ½ä½“å’ŒGraph Agent\n- ğŸ”§ **LLM Providerä½¿ç”¨** - å¤šç§LLMçš„ç»Ÿä¸€è°ƒç”¨\n- ğŸ•¸ï¸ **Graph Agentå®ç°** - åˆ›å»ºç®€å•çš„å›¾ç»“æ„æ™ºèƒ½ä½“\n- ğŸ“Š **åŸºç¡€å®è·µ** - å®Œæˆç®€å•çš„PHMè¯Šæ–­æ¡ˆä¾‹\n\n## ğŸ“‹ é¢„è®¡æ—¶é•¿ï¼š1.5-2å°æ—¶\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# è®¾ç½®é¡¹ç›®è·¯å¾„\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—\n",
    "from modules import agent_basics\n",
    "from modules import llm_providers_unified\n",
    "from modules import graph_agent_intro\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ\")\n",
    "print(f\"é¡¹ç›®æ ¹ç›®å½•: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ä»€ä¹ˆæ˜¯Agentï¼ŸğŸ¤–\n",
    "\n",
    "### 1.1 Agentçš„æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "**Agentï¼ˆæ™ºèƒ½ä½“ï¼‰** æ˜¯ä¸€ä¸ªèƒ½å¤Ÿï¼š\n",
    "- ğŸ“¥ **æ„ŸçŸ¥**ï¼ˆPerceiveï¼‰ï¼šæ¥æ”¶å’Œç†è§£ç¯å¢ƒä¿¡æ¯\n",
    "- ğŸ¤” **æ€è€ƒ**ï¼ˆThinkï¼‰ï¼šå¤„ç†ä¿¡æ¯å¹¶è¿›è¡Œå†³ç­–\n",
    "- ğŸ¬ **è¡ŒåŠ¨**ï¼ˆActï¼‰ï¼šæ‰§è¡Œå†³ç­–å¹¶å½±å“ç¯å¢ƒ\n",
    "\n",
    "è®©æˆ‘ä»¬ä»æœ€ç®€å•çš„Agentå¼€å§‹ç†è§£ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æ¼”ç¤ºåŸºç¡€Agentæ¦‚å¿µ\nprint(\"ğŸ¤– AgentåŸºç¡€æ¦‚å¿µæ¼”ç¤º\")\nagent_basics.demonstrate_agent_basics()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2 ç®€å•Agentç¤ºä¾‹\n\nè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŸºç¡€çš„Agentæ¥ç†è§£å…¶å·¥ä½œåŸç†ï¼š"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# åˆ›å»ºç®€å•Agentç¤ºä¾‹\nfrom modules.agent_basics import SimpleAgent\n\n# æ¼”ç¤ºåŸºæœ¬Agentå·¥ä½œæµç¨‹\nsimple_agent = SimpleAgent()\ntest_input = \"è½´æ‰¿æ¸©åº¦å¼‚å¸¸ï¼Œéœ€è¦è¯Šæ–­\"\nresult = simple_agent.process(test_input)\n\nprint(f\"è¾“å…¥: {test_input}\")\nprint(f\"è¾“å‡º: {result}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Agentçš„å±€é™æ€§\n",
    "\n",
    "ä¼ ç»Ÿçš„è§„åˆ™Agentæœ‰ä»¥ä¸‹å±€é™ï¼š\n",
    "- ğŸ”’ **å›ºå®šè§„åˆ™** - æ— æ³•å¤„ç†æ–°æƒ…å†µ\n",
    "- ğŸ“ **çº¿æ€§æµç¨‹** - æ„ŸçŸ¥â†’æ€è€ƒâ†’è¡ŒåŠ¨çš„å›ºå®šå¾ªç¯\n",
    "- ğŸ§  **æœ‰é™æ™ºèƒ½** - ç¼ºä¹æ¨ç†å’Œå­¦ä¹ èƒ½åŠ›\n",
    "\n",
    "è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æ›´æ™ºèƒ½çš„**Graph Agent**ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2. LLM Provideré›†æˆ - ä¸ºAgentæ·»åŠ \"å¤§è„‘\" ğŸ§ \n\nç°ä»£Agentä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)ä½œä¸º\"å¤§è„‘\"ã€‚è®©æˆ‘ä»¬å­¦ä¹ å¦‚ä½•ä½¿ç”¨ä¸åŒçš„LLM Providerï¼š"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æ£€æŸ¥å¯ç”¨çš„LLM Provider\nfrom modules.llm_providers_unified import UnifiedLLMProvider, create_llm\n\nprint(\"ğŸ” æ£€æŸ¥LLM Providerå¯ç”¨æ€§:\")\navailable_providers = UnifiedLLMProvider.list_available_providers()\n\nfor provider, info in available_providers.items():\n    status = \"âœ…\" if info[\"ready\"] else \"âŒ\"\n    print(f\"{status} {provider}: {info['default_model']}\")\n\n# é€‰æ‹©æœ€ä½³Provider\nbest_provider = UnifiedLLMProvider.get_best_available_provider()\nprint(f\"\\nğŸ¯ ä½¿ç”¨Provider: {best_provider}\")\n\n# æµ‹è¯•LLMè°ƒç”¨\nllm = create_llm(best_provider)\ntest_query = \"ä»€ä¹ˆæ˜¯é¢„æµ‹æ€§å¥åº·ç®¡ç†(PHM)ï¼Ÿ\"\nresponse = llm.invoke(test_query)\nprint(f\"\\nğŸ’¬ æµ‹è¯•æŸ¥è¯¢: {test_query}\")\nprint(f\"ğŸ“ LLMå›ç­”: {response.content[:100]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from modules.llm_providers_unified import create_llm\n",
    "\n",
    "# åˆ›å»ºProvideré€‰æ‹©å™¨\n",
    "available_providers = UnifiedLLMProvider.list_available_providers()\n",
    "ready_providers = [p for p, info in available_providers.items() if info[\"ready\"]]\n",
    "\n",
    "provider_selector = widgets.Dropdown(\n",
    "    options=ready_providers,\n",
    "    value=ready_providers[0] if ready_providers else 'mock',\n",
    "    description='Provider:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "query_input = widgets.Textarea(\n",
    "    value='ä»€ä¹ˆæ˜¯PHMï¼ˆé¢„æµ‹æ€§å¥åº·ç®¡ç†ï¼‰ï¼Ÿè¯·ç®€è¦è§£é‡Šå…¶æ ¸å¿ƒæ¦‚å¿µã€‚',\n",
    "    description='æŸ¥è¯¢å†…å®¹:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='100%', height='80px')\n",
    ")\n",
    "\n",
    "test_button = widgets.Button(\n",
    "    description='ğŸ§ª æµ‹è¯•Provider',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "def test_llm_provider(btn):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            provider = provider_selector.value\n",
    "            print(f\"ğŸ¤– æ­£åœ¨ä½¿ç”¨ {provider} Provider...\")\n",
    "            \n",
    "            # åˆ›å»ºLLMå®ä¾‹\n",
    "            llm = create_llm(provider=provider)\n",
    "            \n",
    "            # å‘é€æŸ¥è¯¢\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            response = llm.invoke(query_input.value)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # æå–å“åº”å†…å®¹\n",
    "            content = response.content if hasattr(response, 'content') else str(response)\n",
    "            \n",
    "            print(f\"\\nâœ… å“åº”æˆåŠŸ (ç”¨æ—¶: {end_time - start_time:.2f}ç§’)\")\n",
    "            print(f\"ğŸ“ å“åº”é•¿åº¦: {len(content)} å­—ç¬¦\")\n",
    "            print(f\"\\nğŸ’¬ {provider} çš„å›ç­”:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ é”™è¯¯: {e}\")\n",
    "            print(\"è¯·æ£€æŸ¥APIå¯†é’¥é…ç½®æˆ–ç½‘ç»œè¿æ¥\")\n",
    "\n",
    "test_button.on_click(test_llm_provider)\n",
    "\n",
    "# æ˜¾ç¤ºç•Œé¢\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>ğŸ§ª LLM Provider äº¤äº’å¼æµ‹è¯•</h3>\"),\n",
    "    provider_selector,\n",
    "    query_input,\n",
    "    test_button,\n",
    "    output\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡ŒLLM ProvideråŸºå‡†æµ‹è¯•\n",
    "print(\"ğŸƒâ€â™‚ï¸ å¼€å§‹LLM ProvideråŸºå‡†æµ‹è¯•...\")\n",
    "results = llm_providers_unified.demo_provider_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æ¼”ç¤ºGraph Agentçš„åŸºæœ¬æ¦‚å¿µ\nfrom modules.graph_agent_intro import SimpleGraphAgent\n\n# åˆ›å»ºç®€å•çš„Graph Agent\nsimple_graph = SimpleGraphAgent()\n\n# è¿è¡ŒGraph Agent\ntest_input = \"è½´æ‰¿æ¸©åº¦ä¼ æ„Ÿå™¨è¯»æ•°å¼‚å¸¸\"\nresult = simple_graph.run(test_input)\n\nprint(f\"ğŸ“Š è¾“å…¥: {test_input}\")\nprint(f\"ğŸ”„ æ‰§è¡Œæµç¨‹: {' â†’ '.join(result['steps'])}\")\nprint(f\"âœ… æœ€ç»ˆç»“æœ: {result.get('result', 'N/A')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from modules.graph_agent_intro import LLMGraphAgent\n\n# åˆ›å»ºLLMå¢å¼ºçš„Graph Agent\nllm_graph = LLMGraphAgent(llm_provider=best_provider)\n\n# æµ‹è¯•PHMåœºæ™¯\nscenario = \"è®¾å¤‡æŒ¯åŠ¨é¢‘ç‡çªç„¶å¢åŠ ï¼Œå¹…åº¦è¶…è¿‡æ­£å¸¸å€¼30%\"\nresult = llm_graph.run(scenario)\n\nprint(f\"ğŸ§ª æµ‹è¯•åœºæ™¯: {scenario}\")\nprint(f\"ğŸ”„ æ‰§è¡Œæ­¥éª¤: {' â†’ '.join(result['steps'])}\")\nprint(f\"ğŸ¯ LLMå†³ç­–: {result.get('result', 'N/A')[:150]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 æ¡ä»¶è·¯ç”±çš„Graph Agent\n",
    "\n",
    "Graph Agentçš„å¼ºå¤§ä¹‹å¤„åœ¨äºèƒ½å¤Ÿæ ¹æ®çŠ¶æ€è¿›è¡Œæ¡ä»¶åˆ†æ”¯ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªPHMè¯Šæ–­çš„ä¾‹å­ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.graph_agent_intro import ConditionalGraphAgent\n",
    "\n",
    "print(\"ğŸŒ¿ åˆ›å»ºæ¡ä»¶è·¯ç”±Graph Agent\")\n",
    "\n",
    "# åˆ›å»ºæ¡ä»¶Graph Agentç”¨äºPHMè¯Šæ–­\n",
    "conditional_graph = ConditionalGraphAgent(llm_provider=best_provider)\n",
    "\n",
    "# æµ‹è¯•ä¸åŒçš„ä¼ æ„Ÿå™¨æ•°æ®åœºæ™¯\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"æ­£å¸¸è¿è¡ŒçŠ¶æ€\",\n",
    "        \"data\": {\"temperature\": 0.45, \"pressure\": 0.52, \"vibration\": 0.48},\n",
    "        \"expected\": \"åº”è¯¥èµ°å¿«é€Ÿæ£€æŸ¥è·¯å¾„\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"è½»å¾®å¼‚å¸¸\",\n",
    "        \"data\": {\"temperature\": 0.75, \"pressure\": 0.65, \"vibration\": 0.58},\n",
    "        \"expected\": \"å¯èƒ½éœ€è¦è¯¦ç»†åˆ†æ\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ä¸¥é‡å¼‚å¸¸\",\n",
    "        \"data\": {\"temperature\": 0.95, \"pressure\": 0.15, \"vibration\": 0.88},\n",
    "        \"expected\": \"éœ€è¦è¯¦ç»†åˆ†æå’Œè¡ŒåŠ¨è§„åˆ’\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for test_case in test_cases:\n",
    "    print(f\"\\n{'ğŸ”¬ ' + test_case['name']:=^60}\")\n",
    "    print(f\"ğŸ“Š ä¼ æ„Ÿå™¨æ•°æ®: {test_case['data']}\")\n",
    "    print(f\"ğŸ“ é¢„æœŸ: {test_case['expected']}\")\n",
    "    \n",
    "    result = conditional_graph.run(test_case[\"data\"])\n",
    "    \n",
    "    print(f\"\\nâœ… å®é™…æ‰§è¡Œè·¯å¾„: {' â†’ '.join(result['steps_completed'])}\")\n",
    "    print(f\"ğŸ“ˆ ç½®ä¿¡åˆ†æ•°: {result.get('confidence_score', 0):.2f}\")\n",
    "    print(f\"ğŸ¥ è¯Šæ–­ç»“æœ: {result.get('diagnosis', 'N/A')[:100]}...\")\n",
    "    \n",
    "    if result.get('action_plan'):\n",
    "        print(f\"ğŸ“‹ è¡ŒåŠ¨è®¡åˆ’: {result['action_plan'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Graph Agentæ¶æ„æ·±åº¦ç†è§£ ğŸ—ï¸\n",
    "\n",
    "### 4.1 Graph Agentçš„æ ¸å¿ƒç»„ä»¶\n",
    "\n",
    "è®©æˆ‘ä»¬æ·±å…¥ç†è§£Graph Agentçš„æ ¸å¿ƒç»„ä»¶ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å±•ç¤ºGraph Agentçš„å†…éƒ¨ç»“æ„\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "import operator\n",
    "\n",
    "# 1. çŠ¶æ€å®šä¹‰ï¼ˆTypedDictï¼‰\n",
    "class PHMDiagnosisState(TypedDict):\n",
    "    \"\"\"PHMè¯Šæ–­çŠ¶æ€å®šä¹‰\"\"\"\n",
    "    sensor_readings: dict  # ä¼ æ„Ÿå™¨è¯»æ•°\n",
    "    analysis_results: Annotated[list, operator.add]  # åˆ†æç»“æœï¼ˆç´¯åŠ ï¼‰\n",
    "    current_step: str  # å½“å‰æ­¥éª¤\n",
    "    confidence_level: float  # ç½®ä¿¡åº¦\n",
    "    recommendations: list  # å»ºè®®åˆ—è¡¨\n",
    "\n",
    "print(\"ğŸ—ï¸ Graph Agentæ ¸å¿ƒç»„ä»¶è§£æ\")\n",
    "print(\"\\n1ï¸âƒ£ çŠ¶æ€å®šä¹‰ï¼ˆTypedDictï¼‰:\")\n",
    "print(\"   - å®šä¹‰Agentåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ç»´æŠ¤çš„æ•°æ®ç»“æ„\")\n",
    "print(\"   - ç±»å‹å®‰å…¨ï¼ŒIDEå‹å¥½\")\n",
    "print(\"   - æ”¯æŒAnnotatedç±»å‹ï¼Œå¦‚è‡ªåŠ¨ç´¯åŠ åˆ—è¡¨\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ èŠ‚ç‚¹å‡½æ•°ï¼ˆNode Functionsï¼‰:\")\n",
    "print(\"   - æ¯ä¸ªèŠ‚ç‚¹æ˜¯ä¸€ä¸ªçº¯å‡½æ•°\")\n",
    "print(\"   - æ¥æ”¶çŠ¶æ€ï¼Œè¿”å›çŠ¶æ€æ›´æ–°\")\n",
    "print(\"   - æ— å‰¯ä½œç”¨ï¼Œæ˜“äºæµ‹è¯•\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ è¾¹å’Œè·¯ç”±ï¼ˆEdges & Routingï¼‰:\")\n",
    "print(\"   - å›ºå®šè¾¹ï¼šadd_edge(from, to)\")\n",
    "print(\"   - æ¡ä»¶è¾¹ï¼šadd_conditional_edges(from, condition_func, mapping)\")\n",
    "print(\"   - æ”¯æŒå¾ªç¯å’Œå¤æ‚æ§åˆ¶æµ\")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ ç¼–è¯‘å’Œæ‰§è¡Œï¼ˆCompile & Invokeï¼‰:\")\n",
    "print(\"   - workflow.compile() åˆ›å»ºå¯æ‰§è¡Œçš„å›¾\")\n",
    "print(\"   - graph.invoke(initial_state) æ‰§è¡Œå·¥ä½œæµ\")\n",
    "print(\"   - è¿”å›æœ€ç»ˆçŠ¶æ€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Dict, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "import operator\n",
    "import time\n",
    "\n",
    "# å®šä¹‰ä¸“ç”¨çš„PHMçŠ¶æ€\n",
    "class PHMDiagnosisState(TypedDict):\n",
    "    sensor_data: Dict[str, float]\n",
    "    analysis_history: Annotated[list, operator.add]\n",
    "    current_diagnosis: str\n",
    "    severity_level: str\n",
    "    recommended_actions: list\n",
    "    processing_time: float\n",
    "\n",
    "class CustomPHMAgent:\n",
    "    \"\"\"è‡ªå®šä¹‰PHMè¯Šæ–­Graph Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_provider: str = \"mock\"):\n",
    "        self.llm = create_llm(llm_provider, temperature=0.3)\n",
    "        self.graph = self._build_phm_graph()\n",
    "    \n",
    "    def _build_phm_graph(self):\n",
    "        \"\"\"æ„å»ºPHMè¯Šæ–­å›¾\"\"\"\n",
    "        \n",
    "        def data_validation_node(state: PHMDiagnosisState) -> Dict[str, Any]:\n",
    "            \"\"\"æ•°æ®éªŒè¯èŠ‚ç‚¹\"\"\"\n",
    "            sensor_data = state[\"sensor_data\"]\n",
    "            \n",
    "            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§\n",
    "            required_sensors = [\"temperature\", \"vibration\", \"pressure\"]\n",
    "            missing_sensors = [s for s in required_sensors if s not in sensor_data]\n",
    "            \n",
    "            if missing_sensors:\n",
    "                validation_result = f\"âš ï¸ ç¼ºå°‘ä¼ æ„Ÿå™¨æ•°æ®: {', '.join(missing_sensors)}\"\n",
    "            else:\n",
    "                validation_result = \"âœ… æ•°æ®éªŒè¯é€šè¿‡\"\n",
    "            \n",
    "            return {\n",
    "                \"analysis_history\": [f\"æ•°æ®éªŒè¯: {validation_result}\"],\n",
    "                \"processing_time\": time.time()\n",
    "            }\n",
    "        \n",
    "        def anomaly_detection_node(state: PHMDiagnosisState) -> Dict[str, Any]:\n",
    "            \"\"\"å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹\"\"\"\n",
    "            sensor_data = state[\"sensor_data\"]\n",
    "            \n",
    "            # ç®€å•çš„é˜ˆå€¼æ£€æµ‹\n",
    "            anomalies = []\n",
    "            thresholds = {\n",
    "                \"temperature\": (0.2, 0.8),\n",
    "                \"vibration\": (0.15, 0.85),  \n",
    "                \"pressure\": (0.25, 0.75)\n",
    "            }\n",
    "            \n",
    "            for sensor, value in sensor_data.items():\n",
    "                if sensor in thresholds:\n",
    "                    low, high = thresholds[sensor]\n",
    "                    if value < low or value > high:\n",
    "                        anomalies.append(f\"{sensor}: {value:.2f}\")\n",
    "            \n",
    "            if anomalies:\n",
    "                severity = \"high\" if len(anomalies) >= 2 else \"medium\"\n",
    "                diagnosis = f\"æ£€æµ‹åˆ°å¼‚å¸¸: {', '.join(anomalies)}\"\n",
    "            else:\n",
    "                severity = \"low\"\n",
    "                diagnosis = \"ç³»ç»Ÿè¿è¡Œæ­£å¸¸\"\n",
    "            \n",
    "            return {\n",
    "                \"analysis_history\": [f\"å¼‚å¸¸æ£€æµ‹: {diagnosis}\"],\n",
    "                \"current_diagnosis\": diagnosis,\n",
    "                \"severity_level\": severity\n",
    "            }\n",
    "        \n",
    "        def llm_analysis_node(state: PHMDiagnosisState) -> Dict[str, Any]:\n",
    "            \"\"\"LLMæ·±åº¦åˆ†æèŠ‚ç‚¹\"\"\"\n",
    "            sensor_data = state[\"sensor_data\"]\n",
    "            current_diagnosis = state[\"current_diagnosis\"]\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "            ä½œä¸ºPHMä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹è¯Šæ–­ä¿¡æ¯ï¼š\n",
    "            \n",
    "            ä¼ æ„Ÿå™¨æ•°æ®: {sensor_data}\n",
    "            åˆæ­¥è¯Šæ–­: {current_diagnosis}\n",
    "            \n",
    "            è¯·æä¾›ï¼š\n",
    "            1. è¯¦ç»†çš„æ ¹å› åˆ†æ\n",
    "            2. é£é™©è¯„ä¼°\n",
    "            3. å…·ä½“çš„ç»´æŠ¤å»ºè®®\n",
    "            \n",
    "            è¯·ç”¨ç®€æ´æ˜äº†çš„ä¸­æ–‡å›ç­”ã€‚\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = self.llm.invoke(prompt)\n",
    "                expert_analysis = response.content if hasattr(response, 'content') else str(response)\n",
    "            except Exception as e:\n",
    "                expert_analysis = f\"LLMåˆ†æå¤±è´¥: {e}\"\n",
    "            \n",
    "            return {\n",
    "                \"analysis_history\": [f\"ä¸“å®¶åˆ†æ: {expert_analysis[:100]}...\"],\n",
    "                \"current_diagnosis\": expert_analysis\n",
    "            }\n",
    "        \n",
    "        def action_planning_node(state: PHMDiagnosisState) -> Dict[str, Any]:\n",
    "            \"\"\"è¡ŒåŠ¨è§„åˆ’èŠ‚ç‚¹\"\"\"\n",
    "            severity = state[\"severity_level\"]\n",
    "            diagnosis = state[\"current_diagnosis\"]\n",
    "            \n",
    "            # åŸºäºä¸¥é‡ç¨‹åº¦åˆ¶å®šè¡ŒåŠ¨è®¡åˆ’\n",
    "            if severity == \"high\":\n",
    "                actions = [\n",
    "                    \"ç«‹å³åœæœºæ£€æŸ¥\",\n",
    "                    \"è”ç³»ç»´æŠ¤å›¢é˜Ÿ\",\n",
    "                    \"å‡†å¤‡å¤‡ç”¨è®¾å¤‡\"\n",
    "                ]\n",
    "            elif severity == \"medium\":\n",
    "                actions = [\n",
    "                    \"å¢åŠ ç›‘æ§é¢‘ç‡\",\n",
    "                    \"å®‰æ’é¢„é˜²æ€§ç»´æŠ¤\",\n",
    "                    \"å‡†å¤‡ç»´æŠ¤è®¡åˆ’\"\n",
    "                ]\n",
    "            else:\n",
    "                actions = [\n",
    "                    \"ç»§ç»­æ­£å¸¸ç›‘æ§\",\n",
    "                    \"è®°å½•å½“å‰çŠ¶æ€\"\n",
    "                ]\n",
    "            \n",
    "            return {\n",
    "                \"analysis_history\": [f\"åˆ¶å®šè¡ŒåŠ¨è®¡åˆ’: {len(actions)}é¡¹è¡ŒåŠ¨\"],\n",
    "                \"recommended_actions\": actions\n",
    "            }\n",
    "        \n",
    "        # æ¡ä»¶è·¯ç”±å‡½æ•°\n",
    "        def should_use_llm(state: PHMDiagnosisState) -> str:\n",
    "            \"\"\"å†³å®šæ˜¯å¦éœ€è¦LLMæ·±åº¦åˆ†æ\"\"\"\n",
    "            severity = state.get(\"severity_level\", \"low\")\n",
    "            return \"llm_analysis\" if severity in [\"medium\", \"high\"] else \"action_planning\"\n",
    "        \n",
    "        # æ„å»ºå›¾ç»“æ„\n",
    "        workflow = StateGraph(PHMDiagnosisState)\n",
    "        \n",
    "        # æ·»åŠ èŠ‚ç‚¹\n",
    "        workflow.add_node(\"validate\", data_validation_node)\n",
    "        workflow.add_node(\"detect\", anomaly_detection_node)\n",
    "        workflow.add_node(\"llm_analysis\", llm_analysis_node)\n",
    "        workflow.add_node(\"plan\", action_planning_node)\n",
    "        \n",
    "        # å®šä¹‰æµç¨‹\n",
    "        workflow.set_entry_point(\"validate\")\n",
    "        workflow.add_edge(\"validate\", \"detect\")\n",
    "        \n",
    "        # æ¡ä»¶è·¯ç”±ï¼šæ ¹æ®ä¸¥é‡ç¨‹åº¦å†³å®šæ˜¯å¦éœ€è¦LLMåˆ†æ\n",
    "        workflow.add_conditional_edges(\n",
    "            \"detect\",\n",
    "            should_use_llm,\n",
    "            {\n",
    "                \"llm_analysis\": \"llm_analysis\",\n",
    "                \"action_planning\": \"plan\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        workflow.add_edge(\"llm_analysis\", \"plan\")\n",
    "        workflow.add_edge(\"plan\", END)\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    def diagnose(self, sensor_data: Dict[str, float]) -> PHMDiagnosisState:\n",
    "        \"\"\"æ‰§è¡ŒPHMè¯Šæ–­\"\"\"\n",
    "        initial_state = {\n",
    "            \"sensor_data\": sensor_data,\n",
    "            \"analysis_history\": [],\n",
    "            \"current_diagnosis\": \"\",\n",
    "            \"severity_level\": \"unknown\",\n",
    "            \"recommended_actions\": [],\n",
    "            \"processing_time\": 0.0\n",
    "        }\n",
    "        \n",
    "        result = self.graph.invoke(initial_state)\n",
    "        return result\n",
    "\n",
    "print(\"ğŸ­ åˆ›å»ºè‡ªå®šä¹‰PHM Graph Agent\")\n",
    "custom_phm_agent = CustomPHMAgent(llm_provider=best_provider)\n",
    "print(\"âœ… è‡ªå®šä¹‰PHM Agentåˆ›å»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„PHMåœºæ™¯\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"name\": \"æ­£å¸¸è¿è¡Œ\",\n",
    "        \"data\": {\"temperature\": 0.45, \"vibration\": 0.50, \"pressure\": 0.48},\n",
    "        \"description\": \"æ‰€æœ‰å‚æ•°åœ¨æ­£å¸¸èŒƒå›´å†…\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"å•ç‚¹å¼‚å¸¸\", \n",
    "        \"data\": {\"temperature\": 0.85, \"vibration\": 0.45, \"pressure\": 0.50},\n",
    "        \"description\": \"æ¸©åº¦åé«˜ï¼Œå…¶ä»–æ­£å¸¸\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"å¤šç‚¹å¼‚å¸¸\",\n",
    "        \"data\": {\"temperature\": 0.92, \"vibration\": 0.88, \"pressure\": 0.15},\n",
    "        \"description\": \"å¤šä¸ªå‚æ•°è¶…å‡ºæ­£å¸¸èŒƒå›´\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"æ•°æ®ä¸å®Œæ•´\",\n",
    "        \"data\": {\"temperature\": 0.65, \"pressure\": 0.40},\n",
    "        \"description\": \"ç¼ºå°‘æŒ¯åŠ¨æ•°æ®\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª æµ‹è¯•è‡ªå®šä¹‰PHM Graph Agent\")\n",
    "\n",
    "for i, scenario in enumerate(test_scenarios, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ”¬ æµ‹è¯•åœºæ™¯ {i}: {scenario['name']}\")\n",
    "    print(f\"ğŸ“ æè¿°: {scenario['description']}\")\n",
    "    print(f\"ğŸ“Š æ•°æ®: {scenario['data']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # æ‰§è¡Œè¯Šæ–­\n",
    "    result = custom_phm_agent.diagnose(scenario['data'])\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print(f\"\\nğŸ”„ æ‰§è¡Œå†å²:\")\n",
    "    for j, step in enumerate(result['analysis_history'], 1):\n",
    "        print(f\"  {j}. {step}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ è¯Šæ–­ç»“æœ:\")\n",
    "    print(f\"  ğŸ¯ è¯Šæ–­: {result['current_diagnosis']}\")\n",
    "    print(f\"  âš¡ ä¸¥é‡ç­‰çº§: {result['severity_level']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ æ¨èè¡ŒåŠ¨:\")\n",
    "    for j, action in enumerate(result['recommended_actions'], 1):\n",
    "        print(f\"  {j}. {action}\")\n",
    "    \n",
    "    print(f\"\\nâ±ï¸ å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç»ƒä¹ ï¼šå®Œå–„è¿™ä¸ªç®€å•çš„è¯Šæ–­Agent\nclass MyDiagnosticAgent:\n    \"\"\"æˆ‘çš„è¯Šæ–­Agent - è¯·å®Œå–„å®ç°\"\"\"\n    \n    def __init__(self, llm_provider=\"mock\"):\n        self.llm = create_llm(llm_provider)\n    \n    def diagnose(self, sensor_data: dict) -> str:\n        \"\"\"è¯Šæ–­è®¾å¤‡çŠ¶æ€ - è¯·å®ç°æ­¤æ–¹æ³•\"\"\"\n        # TODO: æ·»åŠ æ‚¨çš„è¯Šæ–­é€»è¾‘\n        # æç¤ºï¼šå¯ä»¥æ£€æŸ¥sensor_dataä¸­çš„æ¸©åº¦ã€æŒ¯åŠ¨ç­‰å‚æ•°\n        \n        # ç¤ºä¾‹é€»è¾‘ï¼ˆè¯·æ”¹è¿›ï¼‰:\n        if sensor_data.get('temperature', 0) > 80:\n            return \"é«˜æ¸©è­¦å‘Šï¼šå»ºè®®æ£€æŸ¥æ•£çƒ­ç³»ç»Ÿ\"\n        elif sensor_data.get('vibration', 0) > 5:\n            return \"æŒ¯åŠ¨å¼‚å¸¸ï¼šå»ºè®®æ£€æŸ¥è½´æ‰¿\"\n        else:\n            return \"è®¾å¤‡çŠ¶æ€æ­£å¸¸\"\n\n# æµ‹è¯•æ‚¨çš„Agent\nmy_agent = MyDiagnosticAgent(best_provider)\ntest_data = {\"temperature\": 85, \"vibration\": 3.2, \"pressure\": 2.1}\ndiagnosis = my_agent.diagnose(test_data)\n\nprint(f\"ä¼ æ„Ÿå™¨æ•°æ®: {test_data}\")\nprint(f\"è¯Šæ–­ç»“æœ: {diagnosis}\")\nprint(\"\\\\nğŸ’¡ æŒ‘æˆ˜ï¼šå°è¯•æ·»åŠ æ›´å¤æ‚çš„è¯Šæ–­é€»è¾‘ï¼\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ 2ï¼šæ¯”è¾ƒä¸åŒProviderçš„å›ç­”\n",
    "question = \"åœ¨å·¥ä¸šè®¾å¤‡é¢„æµ‹æ€§ç»´æŠ¤ä¸­ï¼Œå¦‚ä½•å¹³è¡¡é¢„è­¦çš„æ•æ„Ÿæ€§å’Œè¯¯æŠ¥ç‡ï¼Ÿ\"\n",
    "\n",
    "# è·å–å¯ç”¨çš„Provider\n",
    "available_providers = UnifiedLLMProvider.list_available_providers()\n",
    "ready_providers = [p for p, info in available_providers.items() if info[\"ready\"]]\n",
    "\n",
    "print(f\"ğŸ¤” é—®é¢˜: {question}\")\n",
    "print(f\"\\nğŸ¤– æµ‹è¯• {len(ready_providers)} ä¸ªProviderçš„å›ç­”:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "responses = {}\n",
    "\n",
    "for provider in ready_providers:\n",
    "    try:\n",
    "        print(f\"\\nğŸš€ {provider.upper()} Provider:\")\n",
    "        llm = create_llm(provider, temperature=0.7)\n",
    "        \n",
    "        import time\n",
    "        start = time.time()\n",
    "        response = llm.invoke(question)\n",
    "        end = time.time()\n",
    "        \n",
    "        content = response.content if hasattr(response, 'content') else str(response)\n",
    "        responses[provider] = {\n",
    "            \"content\": content,\n",
    "            \"time\": end - start,\n",
    "            \"length\": len(content)\n",
    "        }\n",
    "        \n",
    "        print(f\"â±ï¸ å“åº”æ—¶é—´: {end - start:.2f}ç§’\")\n",
    "        print(f\"ğŸ“ å›ç­”é•¿åº¦: {len(content)}å­—ç¬¦\")\n",
    "        print(f\"ğŸ’¬ å›ç­”é¢„è§ˆ: {content[:150]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {provider} å‡ºç°é”™è¯¯: {e}\")\n",
    "        responses[provider] = {\"error\": str(e)}\n",
    "\n",
    "print(f\"\\nğŸ“Š æ¯”è¾ƒæ€»ç»“:\")\n",
    "for provider, resp in responses.items():\n",
    "    if \"error\" not in resp:\n",
    "        print(f\"  {provider}: {resp['time']:.2f}s, {resp['length']}å­—ç¬¦\")\n",
    "    else:\n",
    "        print(f\"  {provider}: é”™è¯¯\")\n",
    "\n",
    "print(\"\\nğŸ¯ æ€è€ƒ: å“ªä¸ªProviderçš„å›ç­”æœ€ç¬¦åˆæ‚¨çš„éœ€æ±‚ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€ç»ˆæ¼”ç¤ºï¼šGraph Agentçš„ä¼˜åŠ¿\n",
    "graph_agent_intro.compare_traditional_vs_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š è¡¥å……èµ„æº\n",
    "\n",
    "### ç›¸å…³æ–‡æ¡£\n",
    "- [LangGraphå®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)\n",
    "- [LangChain Provideré›†æˆ](https://python.langchain.com/docs/integrations/)\n",
    "- [PHMåŸºç¡€æ¦‚å¿µ](https://en.wikipedia.org/wiki/Prognostics_and_health_management)\n",
    "\n",
    "### å¸¸è§é—®é¢˜\n",
    "1. **Q: ä¸ºä»€ä¹ˆé€‰æ‹©Graph Agentè€Œä¸æ˜¯ä¼ ç»ŸAgentï¼Ÿ**\n",
    "   A: Graph Agentæä¾›æ›´å¥½çš„å¯æ‰©å±•æ€§ã€å¯ç»´æŠ¤æ€§å’Œå¤æ‚å†³ç­–èƒ½åŠ›ã€‚\n",
    "\n",
    "2. **Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„LLM Providerï¼Ÿ**\n",
    "   A: è€ƒè™‘å“åº”é€Ÿåº¦ã€æˆæœ¬ã€è¯­è¨€æ”¯æŒå’ŒAPIç¨³å®šæ€§ã€‚\n",
    "\n",
    "3. **Q: Graph Agenté€‚åˆä»€ä¹ˆæ ·çš„åº”ç”¨åœºæ™¯ï¼Ÿ**\n",
    "   A: å¤æ‚å·¥ä½œæµã€å¤šæ­¥å†³ç­–ã€æ¡ä»¶åˆ†æ”¯ã€å¹¶è¡Œå¤„ç†ç­‰åœºæ™¯ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}